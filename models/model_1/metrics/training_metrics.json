{
    "training": [
        {
            "step": 100,
            "logs": {
                "loss": 3.0772,
                "grad_norm": 2.5415332317352295,
                "learning_rate": 0.0004980474851608872,
                "epoch": 0.011715089034676664
            }
        },
        {
            "step": 200,
            "logs": {
                "loss": 2.9641,
                "grad_norm": 2.224560499191284,
                "learning_rate": 0.0004960949703217745,
                "epoch": 0.023430178069353328
            }
        },
        {
            "step": 300,
            "logs": {
                "loss": 2.9,
                "grad_norm": 2.1707069873809814,
                "learning_rate": 0.0004941424554826617,
                "epoch": 0.035145267104029994
            }
        },
        {
            "step": 400,
            "logs": {
                "loss": 2.7634,
                "grad_norm": 2.2945117950439453,
                "learning_rate": 0.0004921899406435489,
                "epoch": 0.046860356138706656
            }
        },
        {
            "step": 500,
            "logs": {
                "loss": 2.7573,
                "grad_norm": 2.821709632873535,
                "learning_rate": 0.0004902374258044361,
                "epoch": 0.05857544517338332
            }
        },
        {
            "step": 600,
            "logs": {
                "loss": 2.7805,
                "grad_norm": 3.625251531600952,
                "learning_rate": 0.0004882849109653233,
                "epoch": 0.07029053420805999
            }
        },
        {
            "step": 700,
            "logs": {
                "loss": 2.7153,
                "grad_norm": 4.410861015319824,
                "learning_rate": 0.0004863519212746017,
                "epoch": 0.08200562324273665
            }
        },
        {
            "step": 800,
            "logs": {
                "loss": 2.7473,
                "grad_norm": 2.8487977981567383,
                "learning_rate": 0.0004843994064354889,
                "epoch": 0.09372071227741331
            }
        },
        {
            "step": 900,
            "logs": {
                "loss": 2.6552,
                "grad_norm": 3.247021436691284,
                "learning_rate": 0.0004824468915963762,
                "epoch": 0.10543580131208997
            }
        },
        {
            "step": 1000,
            "logs": {
                "loss": 2.6531,
                "grad_norm": 3.4625401496887207,
                "learning_rate": 0.0004804943767572634,
                "epoch": 0.11715089034676664
            }
        },
        {
            "step": 1100,
            "logs": {
                "loss": 2.5728,
                "grad_norm": 3.354572057723999,
                "learning_rate": 0.0004785418619181506,
                "epoch": 0.12886597938144329
            }
        },
        {
            "step": 1200,
            "logs": {
                "loss": 2.661,
                "grad_norm": 3.5039639472961426,
                "learning_rate": 0.00047660887222742896,
                "epoch": 0.14058106841611998
            }
        },
        {
            "step": 1300,
            "logs": {
                "loss": 2.6015,
                "grad_norm": 3.787827730178833,
                "learning_rate": 0.00047465635738831616,
                "epoch": 0.15229615745079664
            }
        },
        {
            "step": 1400,
            "logs": {
                "loss": 2.5387,
                "grad_norm": 3.3805348873138428,
                "learning_rate": 0.0004727038425492034,
                "epoch": 0.1640112464854733
            }
        },
        {
            "step": 1500,
            "logs": {
                "loss": 2.5419,
                "grad_norm": 3.36317777633667,
                "learning_rate": 0.0004707513277100906,
                "epoch": 0.17572633552014996
            }
        },
        {
            "step": 1600,
            "logs": {
                "loss": 2.5522,
                "grad_norm": 4.258591651916504,
                "learning_rate": 0.0004687988128709778,
                "epoch": 0.18744142455482662
            }
        },
        {
            "step": 1700,
            "logs": {
                "loss": 2.51,
                "grad_norm": 4.761959075927734,
                "learning_rate": 0.00046684629803186507,
                "epoch": 0.1991565135895033
            }
        },
        {
            "step": 1800,
            "logs": {
                "loss": 2.5217,
                "grad_norm": 3.727017641067505,
                "learning_rate": 0.00046489378319275227,
                "epoch": 0.21087160262417995
            }
        },
        {
            "step": 1900,
            "logs": {
                "loss": 2.5623,
                "grad_norm": 3.381781578063965,
                "learning_rate": 0.00046294126835363947,
                "epoch": 0.2225866916588566
            }
        },
        {
            "step": 2000,
            "logs": {
                "loss": 2.489,
                "grad_norm": 2.881222724914551,
                "learning_rate": 0.0004609887535145267,
                "epoch": 0.23430178069353327
            }
        },
        {
            "step": 2100,
            "logs": {
                "loss": 2.4943,
                "grad_norm": 3.559231996536255,
                "learning_rate": 0.0004590362386754139,
                "epoch": 0.24601686972820994
            }
        },
        {
            "step": 2200,
            "logs": {
                "loss": 2.4951,
                "grad_norm": 3.603713035583496,
                "learning_rate": 0.0004570837238363011,
                "epoch": 0.25773195876288657
            }
        },
        {
            "step": 2300,
            "logs": {
                "loss": 2.4831,
                "grad_norm": 4.104010581970215,
                "learning_rate": 0.00045513120899718843,
                "epoch": 0.2694470477975633
            }
        },
        {
            "step": 2400,
            "logs": {
                "loss": 2.4613,
                "grad_norm": 3.673733949661255,
                "learning_rate": 0.00045317869415807563,
                "epoch": 0.28116213683223995
            }
        },
        {
            "step": 2500,
            "logs": {
                "loss": 2.4748,
                "grad_norm": 6.115017414093018,
                "learning_rate": 0.00045122617931896283,
                "epoch": 0.2928772258669166
            }
        },
        {
            "step": 2600,
            "logs": {
                "loss": 2.4418,
                "grad_norm": 4.156976222991943,
                "learning_rate": 0.0004492736644798501,
                "epoch": 0.3045923149015933
            }
        },
        {
            "step": 2700,
            "logs": {
                "loss": 2.4796,
                "grad_norm": 3.1394004821777344,
                "learning_rate": 0.0004473211496407373,
                "epoch": 0.31630740393626994
            }
        },
        {
            "step": 2800,
            "logs": {
                "loss": 2.4996,
                "grad_norm": 3.8007802963256836,
                "learning_rate": 0.0004453686348016245,
                "epoch": 0.3280224929709466
            }
        },
        {
            "step": 2900,
            "logs": {
                "loss": 2.4132,
                "grad_norm": 3.396369218826294,
                "learning_rate": 0.00044341611996251174,
                "epoch": 0.33973758200562326
            }
        },
        {
            "step": 3000,
            "logs": {
                "loss": 2.4496,
                "grad_norm": 4.945125579833984,
                "learning_rate": 0.00044146360512339894,
                "epoch": 0.3514526710402999
            }
        },
        {
            "step": 3100,
            "logs": {
                "loss": 2.3852,
                "grad_norm": 3.752572536468506,
                "learning_rate": 0.00043951109028428614,
                "epoch": 0.3631677600749766
            }
        },
        {
            "step": 3200,
            "logs": {
                "loss": 2.3953,
                "grad_norm": 4.5954365730285645,
                "learning_rate": 0.0004375585754451734,
                "epoch": 0.37488284910965325
            }
        },
        {
            "step": 3300,
            "logs": {
                "loss": 2.4107,
                "grad_norm": 4.452876567840576,
                "learning_rate": 0.0004356060606060606,
                "epoch": 0.3865979381443299
            }
        },
        {
            "step": 3400,
            "logs": {
                "loss": 2.3995,
                "grad_norm": 3.6116762161254883,
                "learning_rate": 0.00043365354576694785,
                "epoch": 0.3983130271790066
            }
        },
        {
            "step": 3500,
            "logs": {
                "loss": 2.4031,
                "grad_norm": 3.523653984069824,
                "learning_rate": 0.0004317010309278351,
                "epoch": 0.41002811621368324
            }
        },
        {
            "step": 3600,
            "logs": {
                "loss": 2.3718,
                "grad_norm": 3.8339104652404785,
                "learning_rate": 0.0004297485160887223,
                "epoch": 0.4217432052483599
            }
        },
        {
            "step": 3700,
            "logs": {
                "loss": 2.3376,
                "grad_norm": 4.194619178771973,
                "learning_rate": 0.0004277960012496095,
                "epoch": 0.43345829428303656
            }
        },
        {
            "step": 3800,
            "logs": {
                "loss": 2.3649,
                "grad_norm": 5.167135715484619,
                "learning_rate": 0.00042584348641049676,
                "epoch": 0.4451733833177132
            }
        },
        {
            "step": 3900,
            "logs": {
                "loss": 2.3776,
                "grad_norm": 4.886307716369629,
                "learning_rate": 0.00042389097157138396,
                "epoch": 0.4568884723523899
            }
        },
        {
            "step": 4000,
            "logs": {
                "loss": 2.3479,
                "grad_norm": 3.888739585876465,
                "learning_rate": 0.00042193845673227116,
                "epoch": 0.46860356138706655
            }
        },
        {
            "step": 4100,
            "logs": {
                "loss": 2.3838,
                "grad_norm": 4.058990001678467,
                "learning_rate": 0.0004199859418931584,
                "epoch": 0.4803186504217432
            }
        },
        {
            "step": 4200,
            "logs": {
                "loss": 2.3703,
                "grad_norm": 4.686529636383057,
                "learning_rate": 0.0004180334270540456,
                "epoch": 0.49203373945641987
            }
        },
        {
            "step": 4300,
            "logs": {
                "loss": 2.3643,
                "grad_norm": 5.40146017074585,
                "learning_rate": 0.0004160809122149328,
                "epoch": 0.5037488284910965
            }
        },
        {
            "step": 4400,
            "logs": {
                "loss": 2.3662,
                "grad_norm": 3.8870444297790527,
                "learning_rate": 0.0004141479225242112,
                "epoch": 0.5154639175257731
            }
        },
        {
            "step": 4500,
            "logs": {
                "loss": 2.333,
                "grad_norm": 5.332303047180176,
                "learning_rate": 0.0004121954076850984,
                "epoch": 0.5271790065604499
            }
        },
        {
            "step": 4600,
            "logs": {
                "loss": 2.3487,
                "grad_norm": 4.039228439331055,
                "learning_rate": 0.00041024289284598563,
                "epoch": 0.5388940955951266
            }
        },
        {
            "step": 4700,
            "logs": {
                "loss": 2.3046,
                "grad_norm": 4.647162914276123,
                "learning_rate": 0.00040829037800687283,
                "epoch": 0.5506091846298032
            }
        },
        {
            "step": 4800,
            "logs": {
                "loss": 2.3511,
                "grad_norm": 5.383174896240234,
                "learning_rate": 0.0004063378631677601,
                "epoch": 0.5623242736644799
            }
        },
        {
            "step": 4900,
            "logs": {
                "loss": 2.3121,
                "grad_norm": 4.8383965492248535,
                "learning_rate": 0.00040438534832864734,
                "epoch": 0.5740393626991566
            }
        },
        {
            "step": 5000,
            "logs": {
                "loss": 2.2855,
                "grad_norm": 5.076219081878662,
                "learning_rate": 0.00040243283348953454,
                "epoch": 0.5857544517338332
            }
        },
        {
            "step": 5100,
            "logs": {
                "loss": 2.2595,
                "grad_norm": 3.8753325939178467,
                "learning_rate": 0.00040048031865042174,
                "epoch": 0.5974695407685099
            }
        },
        {
            "step": 5200,
            "logs": {
                "loss": 2.3494,
                "grad_norm": 4.643612861633301,
                "learning_rate": 0.000398527803811309,
                "epoch": 0.6091846298031866
            }
        },
        {
            "step": 5300,
            "logs": {
                "loss": 2.3307,
                "grad_norm": 4.3273162841796875,
                "learning_rate": 0.0003965752889721962,
                "epoch": 0.6208997188378632
            }
        },
        {
            "step": 5400,
            "logs": {
                "loss": 2.2733,
                "grad_norm": 4.2318878173828125,
                "learning_rate": 0.0003946227741330834,
                "epoch": 0.6326148078725399
            }
        },
        {
            "step": 5500,
            "logs": {
                "loss": 2.3069,
                "grad_norm": 4.267562389373779,
                "learning_rate": 0.00039267025929397065,
                "epoch": 0.6443298969072165
            }
        },
        {
            "step": 5600,
            "logs": {
                "loss": 2.3252,
                "grad_norm": 3.9141182899475098,
                "learning_rate": 0.00039071774445485785,
                "epoch": 0.6560449859418932
            }
        },
        {
            "step": 5700,
            "logs": {
                "loss": 2.24,
                "grad_norm": 3.6876492500305176,
                "learning_rate": 0.00038876522961574505,
                "epoch": 0.6677600749765699
            }
        },
        {
            "step": 5800,
            "logs": {
                "loss": 2.2614,
                "grad_norm": 4.3677239418029785,
                "learning_rate": 0.0003868127147766323,
                "epoch": 0.6794751640112465
            }
        },
        {
            "step": 5900,
            "logs": {
                "loss": 2.2979,
                "grad_norm": 4.389099597930908,
                "learning_rate": 0.00038486019993751956,
                "epoch": 0.6911902530459232
            }
        },
        {
            "step": 6000,
            "logs": {
                "loss": 2.2969,
                "grad_norm": 4.585690021514893,
                "learning_rate": 0.00038290768509840676,
                "epoch": 0.7029053420805998
            }
        },
        {
            "step": 6100,
            "logs": {
                "loss": 2.2401,
                "grad_norm": 4.842065334320068,
                "learning_rate": 0.000380955170259294,
                "epoch": 0.7146204311152765
            }
        },
        {
            "step": 6200,
            "logs": {
                "loss": 2.2697,
                "grad_norm": 4.155399322509766,
                "learning_rate": 0.0003790026554201812,
                "epoch": 0.7263355201499532
            }
        },
        {
            "step": 6300,
            "logs": {
                "loss": 2.28,
                "grad_norm": 4.409428119659424,
                "learning_rate": 0.0003770501405810684,
                "epoch": 0.7380506091846298
            }
        },
        {
            "step": 6400,
            "logs": {
                "loss": 2.2956,
                "grad_norm": 4.113968372344971,
                "learning_rate": 0.00037509762574195566,
                "epoch": 0.7497656982193065
            }
        },
        {
            "step": 6500,
            "logs": {
                "loss": 2.3111,
                "grad_norm": 4.316977024078369,
                "learning_rate": 0.00037316463605123403,
                "epoch": 0.7614807872539832
            }
        },
        {
            "step": 6600,
            "logs": {
                "loss": 2.2736,
                "grad_norm": 5.337570667266846,
                "learning_rate": 0.00037121212121212123,
                "epoch": 0.7731958762886598
            }
        },
        {
            "step": 6700,
            "logs": {
                "loss": 2.3138,
                "grad_norm": 6.575435638427734,
                "learning_rate": 0.00036925960637300843,
                "epoch": 0.7849109653233365
            }
        },
        {
            "step": 6800,
            "logs": {
                "loss": 2.3015,
                "grad_norm": 4.418577194213867,
                "learning_rate": 0.0003673070915338957,
                "epoch": 0.7966260543580131
            }
        },
        {
            "step": 6900,
            "logs": {
                "loss": 2.2736,
                "grad_norm": 3.757169723510742,
                "learning_rate": 0.0003653545766947829,
                "epoch": 0.8083411433926898
            }
        },
        {
            "step": 7000,
            "logs": {
                "loss": 2.2545,
                "grad_norm": 4.0476460456848145,
                "learning_rate": 0.0003634020618556701,
                "epoch": 0.8200562324273665
            }
        },
        {
            "step": 7100,
            "logs": {
                "loss": 2.2855,
                "grad_norm": 5.351390838623047,
                "learning_rate": 0.00036144954701655734,
                "epoch": 0.8317713214620431
            }
        },
        {
            "step": 7200,
            "logs": {
                "loss": 2.2595,
                "grad_norm": 3.9562110900878906,
                "learning_rate": 0.00035949703217744454,
                "epoch": 0.8434864104967198
            }
        },
        {
            "step": 7300,
            "logs": {
                "loss": 2.2827,
                "grad_norm": 6.00460958480835,
                "learning_rate": 0.0003575445173383318,
                "epoch": 0.8552014995313965
            }
        },
        {
            "step": 7400,
            "logs": {
                "loss": 2.2614,
                "grad_norm": 5.190948486328125,
                "learning_rate": 0.00035559200249921905,
                "epoch": 0.8669165885660731
            }
        },
        {
            "step": 7500,
            "logs": {
                "loss": 2.2787,
                "grad_norm": 4.565726280212402,
                "learning_rate": 0.00035363948766010625,
                "epoch": 0.8786316776007498
            }
        },
        {
            "step": 7600,
            "logs": {
                "loss": 2.2745,
                "grad_norm": 4.300930023193359,
                "learning_rate": 0.00035168697282099345,
                "epoch": 0.8903467666354264
            }
        },
        {
            "step": 7700,
            "logs": {
                "loss": 2.2734,
                "grad_norm": 3.838721513748169,
                "learning_rate": 0.0003497344579818807,
                "epoch": 0.9020618556701031
            }
        },
        {
            "step": 7800,
            "logs": {
                "loss": 2.263,
                "grad_norm": 8.013632774353027,
                "learning_rate": 0.0003477819431427679,
                "epoch": 0.9137769447047798
            }
        },
        {
            "step": 7900,
            "logs": {
                "loss": 2.2659,
                "grad_norm": 3.416085958480835,
                "learning_rate": 0.0003458294283036551,
                "epoch": 0.9254920337394564
            }
        },
        {
            "step": 8000,
            "logs": {
                "loss": 2.2886,
                "grad_norm": 5.524442195892334,
                "learning_rate": 0.00034387691346454235,
                "epoch": 0.9372071227741331
            }
        },
        {
            "step": 8100,
            "logs": {
                "loss": 2.1879,
                "grad_norm": 4.156816482543945,
                "learning_rate": 0.00034192439862542955,
                "epoch": 0.9489222118088098
            }
        },
        {
            "step": 8200,
            "logs": {
                "loss": 2.1845,
                "grad_norm": 4.5861430168151855,
                "learning_rate": 0.00033997188378631676,
                "epoch": 0.9606373008434864
            }
        },
        {
            "step": 8300,
            "logs": {
                "loss": 2.2455,
                "grad_norm": 4.352312088012695,
                "learning_rate": 0.000338019368947204,
                "epoch": 0.9723523898781631
            }
        },
        {
            "step": 8400,
            "logs": {
                "loss": 2.2741,
                "grad_norm": 4.853271484375,
                "learning_rate": 0.0003360668541080912,
                "epoch": 0.9840674789128397
            }
        },
        {
            "step": 8500,
            "logs": {
                "loss": 2.2515,
                "grad_norm": 5.264669418334961,
                "learning_rate": 0.00033411433926897846,
                "epoch": 0.9957825679475164
            }
        },
        {
            "step": 8536,
            "logs": {
                "eval_runtime": 123.7813,
                "eval_samples_per_second": 275.849,
                "eval_steps_per_second": 17.248,
                "epoch": 1.0
            }
        },
        {
            "step": 8600,
            "logs": {
                "loss": 2.288,
                "grad_norm": 4.894635200500488,
                "learning_rate": 0.0003321618244298657,
                "epoch": 1.007497656982193
            }
        },
        {
            "step": 8700,
            "logs": {
                "loss": 2.2138,
                "grad_norm": 4.195326805114746,
                "learning_rate": 0.0003302093095907529,
                "epoch": 1.0192127460168696
            }
        },
        {
            "step": 8800,
            "logs": {
                "loss": 2.1993,
                "grad_norm": 5.136669158935547,
                "learning_rate": 0.0003282763199000313,
                "epoch": 1.0309278350515463
            }
        },
        {
            "step": 8900,
            "logs": {
                "loss": 2.1968,
                "grad_norm": 4.6784563064575195,
                "learning_rate": 0.0003263238050609185,
                "epoch": 1.042642924086223
            }
        },
        {
            "step": 9000,
            "logs": {
                "loss": 2.1975,
                "grad_norm": 4.07340145111084,
                "learning_rate": 0.0003243712902218057,
                "epoch": 1.0543580131208996
            }
        },
        {
            "step": 9100,
            "logs": {
                "loss": 2.1375,
                "grad_norm": 5.800548553466797,
                "learning_rate": 0.00032241877538269294,
                "epoch": 1.0660731021555763
            }
        },
        {
            "step": 9200,
            "logs": {
                "loss": 2.2456,
                "grad_norm": 5.256445407867432,
                "learning_rate": 0.00032046626054358014,
                "epoch": 1.077788191190253
            }
        },
        {
            "step": 9300,
            "logs": {
                "loss": 2.2494,
                "grad_norm": 4.336531162261963,
                "learning_rate": 0.00031851374570446734,
                "epoch": 1.0895032802249296
            }
        },
        {
            "step": 9400,
            "logs": {
                "loss": 2.2354,
                "grad_norm": 5.144009113311768,
                "learning_rate": 0.0003165612308653546,
                "epoch": 1.1012183692596063
            }
        },
        {
            "step": 9500,
            "logs": {
                "loss": 2.2445,
                "grad_norm": 5.502011299133301,
                "learning_rate": 0.0003146087160262418,
                "epoch": 1.1129334582942831
            }
        },
        {
            "step": 9600,
            "logs": {
                "loss": 2.1565,
                "grad_norm": 3.67040753364563,
                "learning_rate": 0.000312656201187129,
                "epoch": 1.1246485473289598
            }
        },
        {
            "step": 9700,
            "logs": {
                "loss": 2.2506,
                "grad_norm": 4.654633522033691,
                "learning_rate": 0.00031070368634801625,
                "epoch": 1.1363636363636362
            }
        },
        {
            "step": 9800,
            "logs": {
                "loss": 2.181,
                "grad_norm": 4.348445415496826,
                "learning_rate": 0.00030875117150890345,
                "epoch": 1.148078725398313
            }
        },
        {
            "step": 9900,
            "logs": {
                "loss": 2.2179,
                "grad_norm": 5.01110315322876,
                "learning_rate": 0.0003067986566697907,
                "epoch": 1.1597938144329896
            }
        },
        {
            "step": 10000,
            "logs": {
                "loss": 2.1605,
                "grad_norm": 4.114377498626709,
                "learning_rate": 0.00030484614183067795,
                "epoch": 1.1715089034676662
            }
        },
        {
            "step": 10100,
            "logs": {
                "loss": 2.2326,
                "grad_norm": 3.791771173477173,
                "learning_rate": 0.00030289362699156515,
                "epoch": 1.1832239925023431
            }
        },
        {
            "step": 10200,
            "logs": {
                "loss": 2.1978,
                "grad_norm": 4.540032863616943,
                "learning_rate": 0.00030094111215245235,
                "epoch": 1.1949390815370198
            }
        },
        {
            "step": 10300,
            "logs": {
                "loss": 2.2654,
                "grad_norm": 4.079665660858154,
                "learning_rate": 0.0002989885973133396,
                "epoch": 1.2066541705716964
            }
        },
        {
            "step": 10400,
            "logs": {
                "loss": 2.1892,
                "grad_norm": 6.181673049926758,
                "learning_rate": 0.0002970360824742268,
                "epoch": 1.218369259606373
            }
        },
        {
            "step": 10500,
            "logs": {
                "loss": 2.2236,
                "grad_norm": 4.535326957702637,
                "learning_rate": 0.000295083567635114,
                "epoch": 1.2300843486410498
            }
        },
        {
            "step": 10600,
            "logs": {
                "loss": 2.1862,
                "grad_norm": 4.103367328643799,
                "learning_rate": 0.00029313105279600126,
                "epoch": 1.2417994376757264
            }
        },
        {
            "step": 10700,
            "logs": {
                "loss": 2.1239,
                "grad_norm": 4.458976745605469,
                "learning_rate": 0.00029117853795688846,
                "epoch": 1.2535145267104029
            }
        },
        {
            "step": 10800,
            "logs": {
                "loss": 2.2411,
                "grad_norm": 4.611008644104004,
                "learning_rate": 0.00028922602311777566,
                "epoch": 1.2652296157450795
            }
        },
        {
            "step": 10900,
            "logs": {
                "loss": 2.112,
                "grad_norm": 4.014966011047363,
                "learning_rate": 0.0002872735082786629,
                "epoch": 1.2769447047797562
            }
        },
        {
            "step": 11000,
            "logs": {
                "loss": 2.1661,
                "grad_norm": 4.2378082275390625,
                "learning_rate": 0.0002853405185879413,
                "epoch": 1.2886597938144329
            }
        },
        {
            "step": 11100,
            "logs": {
                "loss": 2.2407,
                "grad_norm": 5.176579475402832,
                "learning_rate": 0.0002833880037488285,
                "epoch": 1.3003748828491095
            }
        },
        {
            "step": 11200,
            "logs": {
                "loss": 2.1452,
                "grad_norm": 4.970809459686279,
                "learning_rate": 0.0002814354889097157,
                "epoch": 1.3120899718837864
            }
        },
        {
            "step": 11300,
            "logs": {
                "loss": 2.2201,
                "grad_norm": 5.525918960571289,
                "learning_rate": 0.000279482974070603,
                "epoch": 1.323805060918463
            }
        },
        {
            "step": 11400,
            "logs": {
                "loss": 2.2278,
                "grad_norm": 6.615675926208496,
                "learning_rate": 0.0002775304592314902,
                "epoch": 1.3355201499531397
            }
        },
        {
            "step": 11500,
            "logs": {
                "loss": 2.2614,
                "grad_norm": 5.623871326446533,
                "learning_rate": 0.0002755779443923774,
                "epoch": 1.3472352389878164
            }
        },
        {
            "step": 11600,
            "logs": {
                "loss": 2.1848,
                "grad_norm": 5.835113525390625,
                "learning_rate": 0.00027362542955326465,
                "epoch": 1.358950328022493
            }
        },
        {
            "step": 11700,
            "logs": {
                "loss": 2.217,
                "grad_norm": 3.4397833347320557,
                "learning_rate": 0.00027167291471415185,
                "epoch": 1.3706654170571697
            }
        },
        {
            "step": 11800,
            "logs": {
                "loss": 2.2126,
                "grad_norm": 4.224558353424072,
                "learning_rate": 0.00026972039987503905,
                "epoch": 1.3823805060918464
            }
        },
        {
            "step": 11900,
            "logs": {
                "loss": 2.1341,
                "grad_norm": 4.717529773712158,
                "learning_rate": 0.0002677678850359263,
                "epoch": 1.394095595126523
            }
        },
        {
            "step": 12000,
            "logs": {
                "loss": 2.1152,
                "grad_norm": 5.6782355308532715,
                "learning_rate": 0.0002658153701968135,
                "epoch": 1.4058106841611997
            }
        },
        {
            "step": 12100,
            "logs": {
                "loss": 2.1813,
                "grad_norm": 4.407451629638672,
                "learning_rate": 0.0002638628553577007,
                "epoch": 1.4175257731958764
            }
        },
        {
            "step": 12200,
            "logs": {
                "loss": 2.1814,
                "grad_norm": 5.474334239959717,
                "learning_rate": 0.0002619103405185879,
                "epoch": 1.429240862230553
            }
        },
        {
            "step": 12300,
            "logs": {
                "loss": 2.1848,
                "grad_norm": 5.3528289794921875,
                "learning_rate": 0.00025995782567947515,
                "epoch": 1.4409559512652297
            }
        },
        {
            "step": 12400,
            "logs": {
                "loss": 2.243,
                "grad_norm": 5.235598564147949,
                "learning_rate": 0.0002580053108403624,
                "epoch": 1.4526710402999063
            }
        },
        {
            "step": 12500,
            "logs": {
                "loss": 2.1884,
                "grad_norm": 4.841535568237305,
                "learning_rate": 0.0002560527960012496,
                "epoch": 1.464386129334583
            }
        },
        {
            "step": 12600,
            "logs": {
                "loss": 2.2303,
                "grad_norm": 5.45502233505249,
                "learning_rate": 0.00025410028116213686,
                "epoch": 1.4761012183692597
            }
        },
        {
            "step": 12700,
            "logs": {
                "loss": 2.1265,
                "grad_norm": 4.919997215270996,
                "learning_rate": 0.00025214776632302406,
                "epoch": 1.4878163074039363
            }
        },
        {
            "step": 12800,
            "logs": {
                "loss": 2.212,
                "grad_norm": 4.79765510559082,
                "learning_rate": 0.00025019525148391126,
                "epoch": 1.499531396438613
            }
        },
        {
            "step": 12900,
            "logs": {
                "loss": 2.1797,
                "grad_norm": 4.3830108642578125,
                "learning_rate": 0.0002482427366447985,
                "epoch": 1.5112464854732894
            }
        },
        {
            "step": 13000,
            "logs": {
                "loss": 2.1355,
                "grad_norm": 5.160102844238281,
                "learning_rate": 0.00024630974695407683,
                "epoch": 1.522961574507966
            }
        },
        {
            "step": 13100,
            "logs": {
                "loss": 2.1823,
                "grad_norm": 5.406757831573486,
                "learning_rate": 0.0002443572321149641,
                "epoch": 1.5346766635426428
            }
        },
        {
            "step": 13200,
            "logs": {
                "loss": 2.1717,
                "grad_norm": 4.871207237243652,
                "learning_rate": 0.0002424047172758513,
                "epoch": 1.5463917525773194
            }
        },
        {
            "step": 13300,
            "logs": {
                "loss": 2.1735,
                "grad_norm": 5.115520000457764,
                "learning_rate": 0.0002404522024367385,
                "epoch": 1.558106841611996
            }
        },
        {
            "step": 13400,
            "logs": {
                "loss": 2.0866,
                "grad_norm": 6.477055549621582,
                "learning_rate": 0.00023851921274601687,
                "epoch": 1.569821930646673
            }
        },
        {
            "step": 13500,
            "logs": {
                "loss": 2.1098,
                "grad_norm": 4.554823875427246,
                "learning_rate": 0.0002365666979069041,
                "epoch": 1.5815370196813496
            }
        },
        {
            "step": 13600,
            "logs": {
                "loss": 2.213,
                "grad_norm": 3.499234199523926,
                "learning_rate": 0.0002346141830677913,
                "epoch": 1.5932521087160263
            }
        },
        {
            "step": 13700,
            "logs": {
                "loss": 2.2277,
                "grad_norm": 5.895769119262695,
                "learning_rate": 0.00023266166822867855,
                "epoch": 1.604967197750703
            }
        },
        {
            "step": 13800,
            "logs": {
                "loss": 2.1408,
                "grad_norm": 5.769434928894043,
                "learning_rate": 0.00023070915338956578,
                "epoch": 1.6166822867853796
            }
        },
        {
            "step": 13900,
            "logs": {
                "loss": 2.1413,
                "grad_norm": 5.023155212402344,
                "learning_rate": 0.00022875663855045298,
                "epoch": 1.6283973758200563
            }
        },
        {
            "step": 14000,
            "logs": {
                "loss": 2.1137,
                "grad_norm": 4.376203536987305,
                "learning_rate": 0.0002268041237113402,
                "epoch": 1.640112464854733
            }
        },
        {
            "step": 14100,
            "logs": {
                "loss": 2.1549,
                "grad_norm": 4.3528218269348145,
                "learning_rate": 0.00022485160887222744,
                "epoch": 1.6518275538894096
            }
        },
        {
            "step": 14200,
            "logs": {
                "loss": 2.08,
                "grad_norm": 4.264064788818359,
                "learning_rate": 0.00022289909403311464,
                "epoch": 1.6635426429240863
            }
        },
        {
            "step": 14300,
            "logs": {
                "loss": 2.1898,
                "grad_norm": 6.6028618812561035,
                "learning_rate": 0.0002209465791940019,
                "epoch": 1.675257731958763
            }
        },
        {
            "step": 14400,
            "logs": {
                "loss": 2.1687,
                "grad_norm": 3.9227466583251953,
                "learning_rate": 0.00021899406435488912,
                "epoch": 1.6869728209934396
            }
        },
        {
            "step": 14500,
            "logs": {
                "loss": 2.1638,
                "grad_norm": 4.888406753540039,
                "learning_rate": 0.00021704154951577632,
                "epoch": 1.6986879100281163
            }
        },
        {
            "step": 14600,
            "logs": {
                "loss": 2.1678,
                "grad_norm": 4.751535415649414,
                "learning_rate": 0.00021508903467666355,
                "epoch": 1.710402999062793
            }
        },
        {
            "step": 14700,
            "logs": {
                "loss": 2.1369,
                "grad_norm": 4.032752990722656,
                "learning_rate": 0.00021313651983755077,
                "epoch": 1.7221180880974696
            }
        },
        {
            "step": 14800,
            "logs": {
                "loss": 2.1908,
                "grad_norm": 5.936219215393066,
                "learning_rate": 0.00021118400499843797,
                "epoch": 1.7338331771321462
            }
        },
        {
            "step": 14900,
            "logs": {
                "loss": 2.105,
                "grad_norm": 4.044866561889648,
                "learning_rate": 0.00020923149015932523,
                "epoch": 1.745548266166823
            }
        },
        {
            "step": 15000,
            "logs": {
                "loss": 2.1866,
                "grad_norm": 4.907809257507324,
                "learning_rate": 0.00020727897532021245,
                "epoch": 1.7572633552014996
            }
        },
        {
            "step": 15100,
            "logs": {
                "loss": 2.1553,
                "grad_norm": 5.879981517791748,
                "learning_rate": 0.00020532646048109965,
                "epoch": 1.7689784442361762
            }
        },
        {
            "step": 15200,
            "logs": {
                "loss": 2.191,
                "grad_norm": 5.880300045013428,
                "learning_rate": 0.00020337394564198688,
                "epoch": 1.780693533270853
            }
        },
        {
            "step": 15300,
            "logs": {
                "loss": 2.1518,
                "grad_norm": 5.595547676086426,
                "learning_rate": 0.0002014214308028741,
                "epoch": 1.7924086223055296
            }
        },
        {
            "step": 15400,
            "logs": {
                "loss": 2.1534,
                "grad_norm": 6.466241836547852,
                "learning_rate": 0.0001994689159637613,
                "epoch": 1.8041237113402062
            }
        },
        {
            "step": 15500,
            "logs": {
                "loss": 2.1752,
                "grad_norm": 4.299615859985352,
                "learning_rate": 0.00019751640112464856,
                "epoch": 1.8158388003748829
            }
        },
        {
            "step": 15600,
            "logs": {
                "loss": 2.1306,
                "grad_norm": 4.99028205871582,
                "learning_rate": 0.0001955638862855358,
                "epoch": 1.8275538894095595
            }
        },
        {
            "step": 15700,
            "logs": {
                "loss": 2.1707,
                "grad_norm": 6.638576984405518,
                "learning_rate": 0.000193611371446423,
                "epoch": 1.8392689784442362
            }
        },
        {
            "step": 15800,
            "logs": {
                "loss": 2.2003,
                "grad_norm": 5.687094211578369,
                "learning_rate": 0.00019165885660731022,
                "epoch": 1.8509840674789129
            }
        },
        {
            "step": 15900,
            "logs": {
                "loss": 2.1445,
                "grad_norm": 4.3068623542785645,
                "learning_rate": 0.00018970634176819745,
                "epoch": 1.8626991565135895
            }
        },
        {
            "step": 16000,
            "logs": {
                "loss": 2.1817,
                "grad_norm": 3.897714853286743,
                "learning_rate": 0.00018775382692908467,
                "epoch": 1.8744142455482662
            }
        },
        {
            "step": 16100,
            "logs": {
                "loss": 2.148,
                "grad_norm": 4.328420162200928,
                "learning_rate": 0.0001858013120899719,
                "epoch": 1.8861293345829429
            }
        },
        {
            "step": 16200,
            "logs": {
                "loss": 2.2018,
                "grad_norm": 4.292954921722412,
                "learning_rate": 0.00018384879725085913,
                "epoch": 1.8978444236176195
            }
        },
        {
            "step": 16300,
            "logs": {
                "loss": 2.1506,
                "grad_norm": 4.428797721862793,
                "learning_rate": 0.00018189628241174633,
                "epoch": 1.9095595126522962
            }
        },
        {
            "step": 16400,
            "logs": {
                "loss": 2.0623,
                "grad_norm": 4.4526047706604,
                "learning_rate": 0.00017994376757263355,
                "epoch": 1.9212746016869728
            }
        },
        {
            "step": 16500,
            "logs": {
                "loss": 2.1881,
                "grad_norm": 4.395059108734131,
                "learning_rate": 0.00017799125273352078,
                "epoch": 1.9329896907216495
            }
        },
        {
            "step": 16600,
            "logs": {
                "loss": 2.1049,
                "grad_norm": 5.536884784698486,
                "learning_rate": 0.000176038737894408,
                "epoch": 1.9447047797563262
            }
        },
        {
            "step": 16700,
            "logs": {
                "loss": 2.1458,
                "grad_norm": 4.644294738769531,
                "learning_rate": 0.00017408622305529524,
                "epoch": 1.9564198687910028
            }
        },
        {
            "step": 16800,
            "logs": {
                "loss": 2.0761,
                "grad_norm": 5.255547046661377,
                "learning_rate": 0.00017213370821618244,
                "epoch": 1.9681349578256795
            }
        },
        {
            "step": 16900,
            "logs": {
                "loss": 2.1358,
                "grad_norm": 5.8051042556762695,
                "learning_rate": 0.00017018119337706966,
                "epoch": 1.9798500468603561
            }
        },
        {
            "step": 17000,
            "logs": {
                "loss": 2.1186,
                "grad_norm": 5.674991130828857,
                "learning_rate": 0.0001682286785379569,
                "epoch": 1.9915651358950328
            }
        },
        {
            "step": 17072,
            "logs": {
                "eval_runtime": 180.1342,
                "eval_samples_per_second": 189.553,
                "eval_steps_per_second": 11.852,
                "epoch": 2.0
            }
        },
        {
            "step": 17100,
            "logs": {
                "loss": 2.1937,
                "grad_norm": 5.065182209014893,
                "learning_rate": 0.0001662761636988441,
                "epoch": 2.0032802249297093
            }
        },
        {
            "step": 17200,
            "logs": {
                "loss": 2.1949,
                "grad_norm": 5.546896457672119,
                "learning_rate": 0.00016432364885973134,
                "epoch": 2.014995313964386
            }
        },
        {
            "step": 17300,
            "logs": {
                "loss": 2.1714,
                "grad_norm": 5.698789596557617,
                "learning_rate": 0.00016237113402061857,
                "epoch": 2.0267104029990626
            }
        },
        {
            "step": 17400,
            "logs": {
                "loss": 2.1427,
                "grad_norm": 4.937030792236328,
                "learning_rate": 0.00016041861918150577,
                "epoch": 2.0384254920337392
            }
        },
        {
            "step": 17500,
            "logs": {
                "loss": 2.1081,
                "grad_norm": 4.280257701873779,
                "learning_rate": 0.00015848562949078414,
                "epoch": 2.050140581068416
            }
        },
        {
            "step": 17600,
            "logs": {
                "loss": 2.1897,
                "grad_norm": 5.955200672149658,
                "learning_rate": 0.00015653311465167136,
                "epoch": 2.0618556701030926
            }
        },
        {
            "step": 17700,
            "logs": {
                "loss": 2.1357,
                "grad_norm": 5.385845184326172,
                "learning_rate": 0.0001545805998125586,
                "epoch": 2.0735707591377697
            }
        },
        {
            "step": 17800,
            "logs": {
                "loss": 2.151,
                "grad_norm": 5.715409278869629,
                "learning_rate": 0.0001526280849734458,
                "epoch": 2.085285848172446
            }
        },
        {
            "step": 17900,
            "logs": {
                "loss": 2.1098,
                "grad_norm": 3.6161279678344727,
                "learning_rate": 0.00015067557013433302,
                "epoch": 2.097000937207123
            }
        },
        {
            "step": 18000,
            "logs": {
                "loss": 2.1779,
                "grad_norm": 5.530666351318359,
                "learning_rate": 0.00014872305529522024,
                "epoch": 2.108716026241799
            }
        },
        {
            "step": 18100,
            "logs": {
                "loss": 2.0884,
                "grad_norm": 4.326141357421875,
                "learning_rate": 0.00014677054045610747,
                "epoch": 2.1204311152764763
            }
        },
        {
            "step": 18200,
            "logs": {
                "loss": 2.1635,
                "grad_norm": 4.651233673095703,
                "learning_rate": 0.0001448180256169947,
                "epoch": 2.1321462043111525
            }
        },
        {
            "step": 18300,
            "logs": {
                "loss": 2.0819,
                "grad_norm": 5.112088203430176,
                "learning_rate": 0.0001428655107778819,
                "epoch": 2.1438612933458296
            }
        },
        {
            "step": 18400,
            "logs": {
                "loss": 2.1429,
                "grad_norm": 6.527758598327637,
                "learning_rate": 0.00014091299593876913,
                "epoch": 2.155576382380506
            }
        },
        {
            "step": 18500,
            "logs": {
                "loss": 2.1096,
                "grad_norm": 5.463602542877197,
                "learning_rate": 0.00013896048109965635,
                "epoch": 2.167291471415183
            }
        },
        {
            "step": 18600,
            "logs": {
                "loss": 2.0663,
                "grad_norm": 6.233996391296387,
                "learning_rate": 0.00013700796626054358,
                "epoch": 2.179006560449859
            }
        },
        {
            "step": 18700,
            "logs": {
                "loss": 2.0303,
                "grad_norm": 4.179226875305176,
                "learning_rate": 0.0001350554514214308,
                "epoch": 2.1907216494845363
            }
        },
        {
            "step": 18800,
            "logs": {
                "loss": 2.1187,
                "grad_norm": 5.780211448669434,
                "learning_rate": 0.00013310293658231804,
                "epoch": 2.2024367385192125
            }
        },
        {
            "step": 18900,
            "logs": {
                "loss": 2.0208,
                "grad_norm": 4.954870700836182,
                "learning_rate": 0.00013115042174320524,
                "epoch": 2.2141518275538896
            }
        },
        {
            "step": 19000,
            "logs": {
                "loss": 2.0895,
                "grad_norm": 4.055974006652832,
                "learning_rate": 0.00012919790690409246,
                "epoch": 2.2258669165885663
            }
        },
        {
            "step": 19100,
            "logs": {
                "loss": 2.1019,
                "grad_norm": 4.303653240203857,
                "learning_rate": 0.00012724539206497972,
                "epoch": 2.237582005623243
            }
        },
        {
            "step": 19200,
            "logs": {
                "loss": 2.1266,
                "grad_norm": 4.501425743103027,
                "learning_rate": 0.00012529287722586692,
                "epoch": 2.2492970946579196
            }
        },
        {
            "step": 19300,
            "logs": {
                "loss": 2.1114,
                "grad_norm": 3.8757152557373047,
                "learning_rate": 0.00012334036238675414,
                "epoch": 2.2610121836925963
            }
        },
        {
            "step": 19400,
            "logs": {
                "loss": 2.1194,
                "grad_norm": 4.380710124969482,
                "learning_rate": 0.00012138784754764136,
                "epoch": 2.2727272727272725
            }
        },
        {
            "step": 19500,
            "logs": {
                "loss": 2.0829,
                "grad_norm": 5.0784077644348145,
                "learning_rate": 0.00011945485785691972,
                "epoch": 2.2844423617619496
            }
        },
        {
            "step": 19600,
            "logs": {
                "loss": 2.1167,
                "grad_norm": 5.592013359069824,
                "learning_rate": 0.00011750234301780694,
                "epoch": 2.296157450796626
            }
        },
        {
            "step": 19700,
            "logs": {
                "loss": 2.0927,
                "grad_norm": 4.706258296966553,
                "learning_rate": 0.00011554982817869416,
                "epoch": 2.307872539831303
            }
        },
        {
            "step": 19800,
            "logs": {
                "loss": 2.1332,
                "grad_norm": 5.250688076019287,
                "learning_rate": 0.00011359731333958139,
                "epoch": 2.319587628865979
            }
        },
        {
            "step": 19900,
            "logs": {
                "loss": 2.1114,
                "grad_norm": 4.643255233764648,
                "learning_rate": 0.0001116447985004686,
                "epoch": 2.3313027179006562
            }
        },
        {
            "step": 20000,
            "logs": {
                "loss": 2.113,
                "grad_norm": 4.764854431152344,
                "learning_rate": 0.00010969228366135583,
                "epoch": 2.3430178069353325
            }
        },
        {
            "step": 20100,
            "logs": {
                "loss": 2.1019,
                "grad_norm": 4.744647979736328,
                "learning_rate": 0.00010773976882224306,
                "epoch": 2.3547328959700096
            }
        },
        {
            "step": 20200,
            "logs": {
                "loss": 2.0882,
                "grad_norm": 5.657220840454102,
                "learning_rate": 0.00010578725398313027,
                "epoch": 2.3664479850046862
            }
        },
        {
            "step": 20300,
            "logs": {
                "loss": 2.0813,
                "grad_norm": 6.271576404571533,
                "learning_rate": 0.00010383473914401749,
                "epoch": 2.378163074039363
            }
        },
        {
            "step": 20400,
            "logs": {
                "loss": 2.2289,
                "grad_norm": 3.488858938217163,
                "learning_rate": 0.00010188222430490473,
                "epoch": 2.3898781630740396
            }
        },
        {
            "step": 20500,
            "logs": {
                "loss": 2.058,
                "grad_norm": 4.688858509063721,
                "learning_rate": 9.992970946579194e-05,
                "epoch": 2.401593252108716
            }
        },
        {
            "step": 20600,
            "logs": {
                "loss": 2.0903,
                "grad_norm": 4.660844802856445,
                "learning_rate": 9.797719462667915e-05,
                "epoch": 2.413308341143393
            }
        },
        {
            "step": 20700,
            "logs": {
                "loss": 2.098,
                "grad_norm": 5.021270751953125,
                "learning_rate": 9.60246797875664e-05,
                "epoch": 2.4250234301780695
            }
        },
        {
            "step": 20800,
            "logs": {
                "loss": 2.1703,
                "grad_norm": 5.289228439331055,
                "learning_rate": 9.407216494845361e-05,
                "epoch": 2.436738519212746
            }
        },
        {
            "step": 20900,
            "logs": {
                "loss": 2.12,
                "grad_norm": 5.539126396179199,
                "learning_rate": 9.211965010934082e-05,
                "epoch": 2.448453608247423
            }
        },
        {
            "step": 21000,
            "logs": {
                "loss": 2.1417,
                "grad_norm": 4.833462238311768,
                "learning_rate": 9.016713527022806e-05,
                "epoch": 2.4601686972820995
            }
        },
        {
            "step": 21100,
            "logs": {
                "loss": 2.1935,
                "grad_norm": 6.231518745422363,
                "learning_rate": 8.821462043111528e-05,
                "epoch": 2.471883786316776
            }
        },
        {
            "step": 21200,
            "logs": {
                "loss": 2.0299,
                "grad_norm": 4.474226474761963,
                "learning_rate": 8.626210559200249e-05,
                "epoch": 2.483598875351453
            }
        },
        {
            "step": 21300,
            "logs": {
                "loss": 2.0909,
                "grad_norm": 6.533238410949707,
                "learning_rate": 8.430959075288973e-05,
                "epoch": 2.4953139643861295
            }
        },
        {
            "step": 21400,
            "logs": {
                "loss": 2.1196,
                "grad_norm": 5.420933723449707,
                "learning_rate": 8.235707591377694e-05,
                "epoch": 2.5070290534208057
            }
        },
        {
            "step": 21500,
            "logs": {
                "loss": 2.1337,
                "grad_norm": 4.119068622589111,
                "learning_rate": 8.040456107466417e-05,
                "epoch": 2.518744142455483
            }
        },
        {
            "step": 21600,
            "logs": {
                "loss": 2.0623,
                "grad_norm": 5.397777080535889,
                "learning_rate": 7.847157138394252e-05,
                "epoch": 2.530459231490159
            }
        },
        {
            "step": 21700,
            "logs": {
                "loss": 2.0682,
                "grad_norm": 5.140133380889893,
                "learning_rate": 7.651905654482974e-05,
                "epoch": 2.542174320524836
            }
        },
        {
            "step": 21800,
            "logs": {
                "loss": 2.1925,
                "grad_norm": 6.709191799163818,
                "learning_rate": 7.456654170571696e-05,
                "epoch": 2.5538894095595124
            }
        },
        {
            "step": 21900,
            "logs": {
                "loss": 2.095,
                "grad_norm": 4.66639518737793,
                "learning_rate": 7.261402686660419e-05,
                "epoch": 2.5656044985941895
            }
        },
        {
            "step": 22000,
            "logs": {
                "loss": 2.1437,
                "grad_norm": 4.260064601898193,
                "learning_rate": 7.06615120274914e-05,
                "epoch": 2.5773195876288657
            }
        },
        {
            "step": 22100,
            "logs": {
                "loss": 2.142,
                "grad_norm": 4.606982231140137,
                "learning_rate": 6.870899718837863e-05,
                "epoch": 2.589034676663543
            }
        },
        {
            "step": 22200,
            "logs": {
                "loss": 2.1791,
                "grad_norm": 6.4257941246032715,
                "learning_rate": 6.675648234926586e-05,
                "epoch": 2.600749765698219
            }
        },
        {
            "step": 22300,
            "logs": {
                "loss": 2.1477,
                "grad_norm": 5.455758094787598,
                "learning_rate": 6.480396751015307e-05,
                "epoch": 2.612464854732896
            }
        },
        {
            "step": 22400,
            "logs": {
                "loss": 2.1308,
                "grad_norm": 4.75686502456665,
                "learning_rate": 6.28514526710403e-05,
                "epoch": 2.624179943767573
            }
        },
        {
            "step": 22500,
            "logs": {
                "loss": 2.0907,
                "grad_norm": 5.0577473640441895,
                "learning_rate": 6.089893783192752e-05,
                "epoch": 2.6358950328022495
            }
        },
        {
            "step": 22600,
            "logs": {
                "loss": 2.1488,
                "grad_norm": 5.816526889801025,
                "learning_rate": 5.8946422992814746e-05,
                "epoch": 2.647610121836926
            }
        },
        {
            "step": 22700,
            "logs": {
                "loss": 2.1468,
                "grad_norm": 6.525591850280762,
                "learning_rate": 5.6993908153701974e-05,
                "epoch": 2.659325210871603
            }
        },
        {
            "step": 22800,
            "logs": {
                "loss": 2.0626,
                "grad_norm": 5.913523197174072,
                "learning_rate": 5.504139331458919e-05,
                "epoch": 2.6710402999062794
            }
        },
        {
            "step": 22900,
            "logs": {
                "loss": 2.0706,
                "grad_norm": 4.109978199005127,
                "learning_rate": 5.3088878475476415e-05,
                "epoch": 2.682755388940956
            }
        },
        {
            "step": 23000,
            "logs": {
                "loss": 2.1374,
                "grad_norm": 4.36393928527832,
                "learning_rate": 5.113636363636364e-05,
                "epoch": 2.6944704779756328
            }
        },
        {
            "step": 23100,
            "logs": {
                "loss": 2.1141,
                "grad_norm": 5.51154088973999,
                "learning_rate": 4.918384879725086e-05,
                "epoch": 2.7061855670103094
            }
        },
        {
            "step": 23200,
            "logs": {
                "loss": 2.0901,
                "grad_norm": 5.307064056396484,
                "learning_rate": 4.723133395813808e-05,
                "epoch": 2.717900656044986
            }
        },
        {
            "step": 23300,
            "logs": {
                "loss": 2.0653,
                "grad_norm": 6.016906261444092,
                "learning_rate": 4.52788191190253e-05,
                "epoch": 2.7296157450796628
            }
        },
        {
            "step": 23400,
            "logs": {
                "loss": 2.1254,
                "grad_norm": 4.460589408874512,
                "learning_rate": 4.332630427991253e-05,
                "epoch": 2.7413308341143394
            }
        },
        {
            "step": 23500,
            "logs": {
                "loss": 2.096,
                "grad_norm": 5.075941562652588,
                "learning_rate": 4.137378944079975e-05,
                "epoch": 2.753045923149016
            }
        },
        {
            "step": 23600,
            "logs": {
                "loss": 2.0874,
                "grad_norm": 5.366241931915283,
                "learning_rate": 3.942127460168697e-05,
                "epoch": 2.7647610121836927
            }
        },
        {
            "step": 23700,
            "logs": {
                "loss": 2.1094,
                "grad_norm": 3.834813356399536,
                "learning_rate": 3.748828491096532e-05,
                "epoch": 2.7764761012183694
            }
        },
        {
            "step": 23800,
            "logs": {
                "loss": 2.0597,
                "grad_norm": 5.32538366317749,
                "learning_rate": 3.553577007185255e-05,
                "epoch": 2.788191190253046
            }
        },
        {
            "step": 23900,
            "logs": {
                "loss": 2.0213,
                "grad_norm": 4.812305927276611,
                "learning_rate": 3.358325523273977e-05,
                "epoch": 2.7999062792877227
            }
        },
        {
            "step": 24000,
            "logs": {
                "loss": 2.1097,
                "grad_norm": 5.2542266845703125,
                "learning_rate": 3.163074039362699e-05,
                "epoch": 2.8116213683223994
            }
        },
        {
            "step": 24100,
            "logs": {
                "loss": 2.1072,
                "grad_norm": 5.15362024307251,
                "learning_rate": 2.9678225554514217e-05,
                "epoch": 2.823336457357076
            }
        },
        {
            "step": 24200,
            "logs": {
                "loss": 2.032,
                "grad_norm": 4.822260856628418,
                "learning_rate": 2.7725710715401437e-05,
                "epoch": 2.8350515463917527
            }
        },
        {
            "step": 24300,
            "logs": {
                "loss": 2.1661,
                "grad_norm": 4.33353853225708,
                "learning_rate": 2.5773195876288658e-05,
                "epoch": 2.8467666354264294
            }
        },
        {
            "step": 24400,
            "logs": {
                "loss": 2.0969,
                "grad_norm": 4.216516017913818,
                "learning_rate": 2.3820681037175885e-05,
                "epoch": 2.858481724461106
            }
        },
        {
            "step": 24500,
            "logs": {
                "loss": 2.0922,
                "grad_norm": 5.185056209564209,
                "learning_rate": 2.1868166198063105e-05,
                "epoch": 2.8701968134957827
            }
        },
        {
            "step": 24600,
            "logs": {
                "loss": 2.0879,
                "grad_norm": 5.646785736083984,
                "learning_rate": 1.991565135895033e-05,
                "epoch": 2.8819119025304594
            }
        },
        {
            "step": 24700,
            "logs": {
                "loss": 2.0785,
                "grad_norm": 4.684732913970947,
                "learning_rate": 1.796313651983755e-05,
                "epoch": 2.893626991565136
            }
        },
        {
            "step": 24800,
            "logs": {
                "loss": 2.1062,
                "grad_norm": 6.164306163787842,
                "learning_rate": 1.6010621680724773e-05,
                "epoch": 2.9053420805998127
            }
        },
        {
            "step": 24900,
            "logs": {
                "loss": 2.1038,
                "grad_norm": 5.611782073974609,
                "learning_rate": 1.4058106841611997e-05,
                "epoch": 2.9170571696344894
            }
        },
        {
            "step": 25000,
            "logs": {
                "loss": 2.1007,
                "grad_norm": 5.8781867027282715,
                "learning_rate": 1.210559200249922e-05,
                "epoch": 2.928772258669166
            }
        },
        {
            "step": 25100,
            "logs": {
                "loss": 2.0692,
                "grad_norm": 5.955507755279541,
                "learning_rate": 1.0153077163386442e-05,
                "epoch": 2.9404873477038427
            }
        },
        {
            "step": 25200,
            "logs": {
                "loss": 2.0915,
                "grad_norm": 4.0433197021484375,
                "learning_rate": 8.200562324273665e-06,
                "epoch": 2.9522024367385193
            }
        },
        {
            "step": 25300,
            "logs": {
                "loss": 2.1242,
                "grad_norm": 5.225831031799316,
                "learning_rate": 6.2480474851608875e-06,
                "epoch": 2.963917525773196
            }
        },
        {
            "step": 25400,
            "logs": {
                "loss": 1.9963,
                "grad_norm": 5.1370697021484375,
                "learning_rate": 4.2955326460481105e-06,
                "epoch": 2.9756326148078727
            }
        },
        {
            "step": 25500,
            "logs": {
                "loss": 2.122,
                "grad_norm": 6.708714485168457,
                "learning_rate": 2.3430178069353326e-06,
                "epoch": 2.9873477038425493
            }
        },
        {
            "step": 25600,
            "logs": {
                "loss": 2.0976,
                "grad_norm": 7.030633449554443,
                "learning_rate": 3.9050296782255547e-07,
                "epoch": 2.999062792877226
            }
        },
        {
            "step": 25608,
            "logs": {
                "eval_runtime": 155.4032,
                "eval_samples_per_second": 219.719,
                "eval_steps_per_second": 13.738,
                "epoch": 3.0
            }
        },
        {
            "step": 25608,
            "logs": {
                "train_runtime": 4267.5007,
                "train_samples_per_second": 96.011,
                "train_steps_per_second": 6.001,
                "total_flos": 2.7106008363270144e+16,
                "train_loss": 2.2329676974307593,
                "epoch": 3.0
            }
        }
    ],
    "evaluation": [
        {
            "step": 8536,
            "metrics": {
                "eval_runtime": 123.7813,
                "eval_samples_per_second": 275.849,
                "eval_steps_per_second": 17.248,
                "epoch": 1.0
            }
        },
        {
            "step": 17072,
            "metrics": {
                "eval_runtime": 180.1342,
                "eval_samples_per_second": 189.553,
                "eval_steps_per_second": 11.852,
                "epoch": 2.0
            }
        },
        {
            "step": 25608,
            "metrics": {
                "eval_runtime": 155.4032,
                "eval_samples_per_second": 219.719,
                "eval_steps_per_second": 13.738,
                "epoch": 3.0
            }
        }
    ]
}