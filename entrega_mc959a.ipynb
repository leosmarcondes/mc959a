{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação de Vulnerabilidades utilizando Aprendizado Auto-Supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grupo : Classificação de Vulnerabilidades utilizando Aprendizado Auto-Supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Install necessary libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdown scikit-learn transformers pinecone unidecode torch peft datasets matplotlib python-dotenv ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Import necessary libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics and performance evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, classification_report  # Tools for evaluating classification models\n",
    "\n",
    "# Hugging Face Transformers for pre-trained models and tokenization\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, TrainerCallback  # Pre-trained models and training utilities\n",
    "from transformers import DataCollatorForLanguageModeling  # Data collation for masked language modeling tasks\n",
    "\n",
    "# Parameter-efficient fine-tuning (PEFT) tools\n",
    "from peft import LoraConfig, get_peft_model, PeftModel  # LoRa configurations and PEFT model utilities\n",
    "\n",
    "# Data handling and preprocessing\n",
    "from torch.utils.data import DataLoader, TensorDataset  # PyTorch data handling utilities\n",
    "from sklearn.model_selection import train_test_split  # Train-test splitting\n",
    "from sklearn.preprocessing import LabelEncoder  # Encoding target labels\n",
    "\n",
    "# Pinecone for vector database operations\n",
    "from pinecone import ServerlessSpec, Pinecone  # Pinecone serverless setup and API access\n",
    "\n",
    "# Datasets and visualizations\n",
    "from datasets import load_dataset  # Loading pre-built datasets\n",
    "from sklearn.manifold import TSNE  # Dimensionality reduction for visualization\n",
    "import matplotlib.pyplot as plt  # Plotting utilities\n",
    "\n",
    "# Environment variable management\n",
    "from dotenv import load_dotenv  # Load environment variables from a .env file\n",
    "\n",
    "# PyTorch and related utilities\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch  # Core PyTorch library\n",
    "\n",
    "# File management and downloading\n",
    "import gdown  # Download files from Google Drive\n",
    "import json  # Handle JSON data\n",
    "import os  # OS-level operations\n",
    "\n",
    "# Miscellaneous utilities\n",
    "import unidecode\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data handling with DataFrames\n",
    "from tqdm.auto import tqdm  # Progress bar for loops\n",
    "import time  # Time measurement\n",
    "import re  # Regular expression operations\n",
    "\n",
    "# Ollama API\n",
    "import ollama  # Interaction with the Ollama library for model inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Developed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The fine-tuning process was conducted using the \"roberta-base\" model, a derivative of BERT developed by Facebook. Fine-tuning employed\n",
    "> the LoRa (Low-Rank Adaptation) technique with a rank-8 matrix to accommodate computational resource constraints. The dataset consisted of CVE descriptions curated by the Mitre Corporation.\n",
    ">\n",
    "> The fine-tuned model was used to generate a vector database hosted on Pinecone, containing representations of the 25 most prevalent CWEs from the Mitre database. For retrieval, a top_k=10 strategy was applied, ensuring the most relevant contexts were selected for classification.\n",
    "> \n",
    "> To enhance classification accuracy, an example builder was implemented to generate question-and-answer pairs illustrating correct CVE-to-CWE mappings. This was integrated into a Retrieval-Augmented Generation (RAG) strategy, where the vector database provided context information, and the examples served as in-context learning data for a Llama model.\n",
    "> \n",
    "> This system enables the Llama model to accurately predict the appropriate CWE category for a given CVE description by leveraging both contextual and example-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Base-model fine-tunning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1. Extract, transform and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download\n",
    "This section ensures that the necessary dataset is downloaded and stored locally. The dataset is available via Google Drive, \n",
    "and the `gdown` library is used for downloading. \n",
    "\n",
    "The dataset is saved in the following location:\n",
    "- **Directory**: `./app/data`\n",
    "- **Filename**: `cve_dataset.csv`\n",
    "\n",
    "If the directory does not exist, it will be created automatically. The dataset URL and its purpose are provided for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory and dataset URL\n",
    "data_dir = \"./app/data\"\n",
    "cve_dataset_url_download = \"https://drive.google.com/uc?id=1vGQWNhupeN08fvtuHEBBuhH8sef8XJUu\"\n",
    "cve_dataset_path = os.path.join(data_dir, \"cve_dataset.csv\")\n",
    "\n",
    "# Create the data directory if it does not exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download the dataset from the provided URL\n",
    "# Arguments:\n",
    "# - `cve_dataset_url_download` (str): The URL of the dataset to download.\n",
    "# - `cve_dataset_path` (str): The local file path where the dataset will be saved.\n",
    "# Returns:\n",
    "# - The file path of the downloaded dataset.\n",
    "gdown.download(cve_dataset_url_download, cve_dataset_path, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "This section processes the CVE dataset to ensure it is clean and focused on relevant information. The steps include:\n",
    "1. **Year extraction and filtering**: Only CVEs reported after the year 2000 are considered.\n",
    "2. **Removing missing descriptions**: CVEs without descriptions are excluded as they are essential for analysis.\n",
    "3. **Dropping unnecessary columns**: Columns `cwe` and `year` are removed to focus on the `cve_id` and `description`.\n",
    "4. **Resetting the DataFrame index**: This ensures a clean, continuous index after filtering.\n",
    "\n",
    "#### CWE Codes of Interest\n",
    "The predefined list of CWE codes (`cwe_list`) includes common software weaknesses, which may be referenced in later steps of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of CWE codes of interest\n",
    "# Arguments:\n",
    "# - `cwe_list` (list): List of CWE codes that may be used in future steps for filtering or analysis.\n",
    "# Purpose:\n",
    "# - This ensures we have a predefined focus on specific types of vulnerabilities.\n",
    "cwe_list = [\n",
    "    \"CWE-79\", \"CWE-787\", \"CWE-89\", \"CWE-352\", \"CWE-22\", \"CWE-125\", \"CWE-78\",\n",
    "    \"CWE-416\", \"CWE-862\", \"CWE-434\", \"CWE-94\", \"CWE-20\", \"CWE-77\", \"CWE-287\",\n",
    "    \"CWE-269\", \"CWE-502\", \"CWE-200\", \"CWE-863\", \"CWE-918\", \"CWE-119\", \"CWE-476\",\n",
    "    \"CWE-798\", \"CWE-190\", \"CWE-400\", \"CWE-306\"\n",
    "]\n",
    "\n",
    "# Read the CVE dataset\n",
    "# Arguments:\n",
    "# - `cve_dataset_path` (str): Path to the CSV file containing the CVE dataset.\n",
    "# Returns:\n",
    "# - A pandas DataFrame containing the loaded dataset.\n",
    "cve_dataset = pd.read_csv(cve_dataset_path)\n",
    "\n",
    "# Extract the year from the CVE ID and filter the dataset\n",
    "# The CVE ID format is \"CVE-YYYY-XXXX\", so the second part is the year.\n",
    "# Only consider entries with years greater than 2000.\n",
    "cve_dataset['year'] = cve_dataset['cve_id'].apply(lambda x: int(x.split('-')[1]))\n",
    "cve_dataset = cve_dataset[cve_dataset['year'] > 2000]\n",
    "\n",
    "# Remove entries with missing descriptions\n",
    "# The 'description' column contains text describing the vulnerability.\n",
    "cve_dataset.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns to focus on relevant data\n",
    "# - 'cwe': The CWE column is dropped because it is not used in subsequent analysis.\n",
    "# - 'year': The year column is no longer needed after filtering.\n",
    "cve_dataset.drop(columns=['cwe', 'year'], inplace=True)\n",
    "\n",
    "# Reset the index to ensure a clean DataFrame\n",
    "cve_dataset.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2. Split dataset for LoRa fine-tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and Storage\n",
    "This section splits the preprocessed dataset into training and testing sets for fine-tuning models. The steps include:\n",
    "1. **Splitting the dataset**: The data is divided into 80% for training and 20% for testing, ensuring reproducibility with a fixed random seed.\n",
    "2. **Saving to CSV**: The `description` column is saved as plain text in the following structure:\n",
    "    - **Training dataset**: `./datasets/finetune/train.csv`\n",
    "    - **Testing dataset**: `./datasets/finetune/test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for storing fine-tuning datasets\n",
    "# Arguments:\n",
    "# - Directory path: `./datasets/finetune`\n",
    "# Purpose:\n",
    "# - Ensure the directory structure is in place for saving the train and test datasets.\n",
    "os.makedirs(\"./datasets/finetune\", exist_ok=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# Arguments:\n",
    "# - `cve_dataset` (DataFrame): The filtered and preprocessed CVE dataset.\n",
    "# - `test_size` (float): Proportion of the dataset to include in the test split (20% here).\n",
    "# - `random_state` (int): Random seed to ensure reproducibility of the split.\n",
    "# Returns:\n",
    "# - `cve_train_data` (DataFrame): Training portion of the dataset.\n",
    "# - `cve_test_data` (DataFrame): Testing portion of the dataset.\n",
    "cve_train_data, cve_test_data = train_test_split(\n",
    "    cve_dataset,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Print the sizes of the training and testing datasets for validation\n",
    "print(f\"Training set size: {len(cve_train_data)}\")\n",
    "print(f\"Testing set size: {len(cve_test_data)}\")\n",
    "\n",
    "# Save the training and testing datasets to CSV files\n",
    "# The descriptions are extracted and saved as plain text for fine-tuning purposes.\n",
    "# Arguments:\n",
    "# - `index=False`: Avoids saving the DataFrame index.\n",
    "# - `header=False`: Ensures only text is saved, suitable for certain NLP frameworks.\n",
    "\n",
    "# Save training dataset\n",
    "cve_train_data['description'].to_csv(\"./datasets/finetune/train.csv\", index=False, header=False)\n",
    "# Save testing dataset\n",
    "cve_test_data['description'].to_csv(\"./datasets/finetune/test.csv\", index=False, header=False)\n",
    "\n",
    "# Save only the 'text' column as .txt files for model consumption\n",
    "cve_train_data['description'].to_csv(f\"./datasets/finetune/train.txt\", index=False, header=False)\n",
    "cve_test_data['description'].to_csv(f\"./datasets/finetune/test.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3. LoRa Fine-tunning Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning with LoRA\n",
    "This section fine-tunes the RoBERTa model using LoRA for masked language modeling. Key steps include:\n",
    "1. **Model and LoRA configuration**: The base model is adapted with LoRA for efficient fine-tuning.\n",
    "2. **Dataset preparation**: Training and testing datasets are tokenized with padding and truncation.\n",
    "3. **Training**: The model is trained using a Trainer object with mixed-precision training enabled.\n",
    "4. **Metrics and Saving**: Training metrics are logged, and the fine-tuned model is saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the version for model outputs\n",
    "# Arguments:\n",
    "# - `MODEL_VERSION` (int): Version number for the fine-tuned model.\n",
    "MODEL_VERSION = 1\n",
    "output_dir = f\"./models/model_{MODEL_VERSION}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define base model and tokenizer\n",
    "# The RoBERTa base model is used for masked language modeling (MLM).\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# LoRA Configuration\n",
    "# LoRA adapts specific layers of the base model with low-rank modifications for efficient fine-tuning.\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"MLM\",  # Task: Masked Language Modeling\n",
    "    r=8,              # Rank\n",
    "    lora_alpha=16,    # Scaling factor\n",
    "    lora_dropout=0.1, # Dropout rate for LoRA\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # Targeted attention layers\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "# The dataset is loaded from the previously saved text files.\n",
    "# Arguments:\n",
    "# - `data_files`: Paths to train and test datasets.\n",
    "dataset = load_dataset(\n",
    "    \"text\", \n",
    "    data_files={\"train\": \"./datasets/finetune/train.txt\", \"test\": \"./datasets/finetune/test.txt\"}\n",
    ")\n",
    "\n",
    "# Preprocessing function for tokenizing text\n",
    "# Arguments:\n",
    "# - `examples` (dict): Dictionary containing text data.\n",
    "# Returns:\n",
    "# - Tokenized dataset with padding and truncation.\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator for dynamic masking during training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,  # Probability of masking tokens\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{output_dir}/results_lora\",  # Directory for training results\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=5e-4,  # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    save_total_limit=2,  # Limit on saved models\n",
    "    logging_steps=100,  # Logging frequency\n",
    "    save_steps=500,  # Save model every 500 steps\n",
    "    report_to=\"none\",  # Disable reporting to external platforms\n",
    "    fp16=True,  # Enable mixed-precision training\n",
    ")\n",
    "\n",
    "# Callback for saving metrics\n",
    "class SaveMetricsCallback(TrainerCallback):\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.metrics = {\"training\": [], \"evaluation\": []}\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            self.metrics[\"training\"].append({\"step\": state.global_step, \"logs\": logs})\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            self.metrics[\"evaluation\"].append({\"step\": state.global_step, \"metrics\": metrics})\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        metrics_file = os.path.join(self.output_dir, \"training_metrics.json\")\n",
    "        with open(metrics_file, \"w\") as f:\n",
    "            json.dump(self.metrics, f, indent=4)\n",
    "        print(f\"Saved metrics to {metrics_file}\")\n",
    "\n",
    "# Initialize callback for saving metrics\n",
    "metrics_output_dir = f\"{output_dir}/metrics\"\n",
    "os.makedirs(metrics_output_dir, exist_ok=True)\n",
    "save_metrics_callback = SaveMetricsCallback(metrics_output_dir)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[save_metrics_callback],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(f\"{output_dir}/fine_tuned_lora_mlm\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/fine_tuned_lora_mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4. Trainning graphics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Metrics Visualization\n",
    "This section visualizes key metrics from the training logs:\n",
    "1. **Training Loss**: Tracks the model's loss function value over training steps.\n",
    "2. **Gradient Norm**: Visualizes the norm of gradients, which provides insights into the stability of the training process.\n",
    "3. **Learning Rate**: Shows the learning rate schedule during training.\n",
    "\n",
    "Each plot provides:\n",
    "- **Steps (x-axis)**: The training step where the metric was logged.\n",
    "- **Metric value (y-axis)**: The corresponding value of the metric.\n",
    "\n",
    "#### Observations\n",
    "- **Loss**: Should generally decrease over steps, indicating model improvement.\n",
    "- **Grad Norm**: Large spikes may indicate instability in gradients.\n",
    "- **Learning Rate**: Should follow the defined schedule, aiding in optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the training state JSON file\n",
    "# This file contains logs from the training process, including loss, gradient norms, and learning rates.\n",
    "file_path = f\"./models/model_{MODEL_VERSION}/results_lora/checkpoint-25608/trainer_state.json\"\n",
    "\n",
    "# Load the JSON data\n",
    "# Arguments:\n",
    "# - `file_path` (str): Path to the `trainer_state.json` file.\n",
    "# Returns:\n",
    "# - `data` (dict): Parsed JSON data containing training logs.\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract relevant metrics from the log history\n",
    "# - `steps`: Training steps where metrics were logged.\n",
    "# - `losses`: Training loss values at corresponding steps.\n",
    "# - `grad_norms`: Gradient norms (if available) at corresponding steps.\n",
    "# - `learning_rates`: Learning rates at corresponding steps.\n",
    "log_history = data.get(\"log_history\", [])\n",
    "steps = [entry[\"step\"] for entry in log_history if \"loss\" in entry]\n",
    "losses = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\n",
    "grad_norms = [entry[\"grad_norm\"] for entry in log_history if \"grad_norm\" in entry]\n",
    "learning_rates = [entry[\"learning_rate\"] for entry in log_history if \"learning_rate\" in entry]\n",
    "\n",
    "# Visualization of training metrics\n",
    "# Create three subplots: Training Loss, Gradient Norm, and Learning Rate over training steps.\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "axes[0].plot(steps, losses, label=\"Training Loss\", color=\"orange\")\n",
    "axes[0].set_title(\"Training Loss Over Steps\")\n",
    "axes[0].set_xlabel(\"Steps\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot gradient norms\n",
    "axes[1].plot(steps[:len(grad_norms)], grad_norms, label=\"Grad Norm\", color=\"blue\")\n",
    "axes[1].set_title(\"Grad Norm Over Steps\")\n",
    "axes[1].set_xlabel(\"Steps\")\n",
    "axes[1].set_ylabel(\"Grad Norm\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot learning rates\n",
    "axes[2].plot(steps[:len(learning_rates)], learning_rates, label=\"Learning Rate\", color=\"green\")\n",
    "axes[2].set_title(\"Learning Rate Over Steps\")\n",
    "axes[2].set_xlabel(\"Steps\")\n",
    "axes[2].set_ylabel(\"Learning Rate\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1. Loading base and fine-tunned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Base and Fine-Tuned Models\n",
    "This section initializes the base model (`roberta-base`) and loads the fine-tuned LoRA adapter for inference or further analysis.\n",
    "\n",
    "#### Steps:\n",
    "1. **Base Model**:\n",
    "    - The base model, RoBERTa, is loaded with its tokenizer.\n",
    "    - Hidden states output is enabled for advanced downstream tasks.\n",
    "2. **Fine-Tuned Model**:\n",
    "    - The LoRA adapter, fine-tuned for masked language modeling, is loaded and integrated with the base model.\n",
    "    - A separate tokenizer associated with the fine-tuned model is initialized.\n",
    "\n",
    "#### Paths:\n",
    "- **Base Model**: `roberta-base` from Hugging Face model hub.\n",
    "- **Fine-Tuned Model**: Located at `./models/model_{MODEL_VERSION}/fine_tuned_lora_mlm`.\n",
    "\n",
    "These models can now be used for tasks such as inference or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base model and tokenizer\n",
    "# Arguments:\n",
    "# - `base_model_name` (str): Name of the base model from Hugging Face's model hub.\n",
    "# Returns:\n",
    "# - `tokenizer_bm`: Tokenizer for the base model.\n",
    "# - `base_model`: Pretrained RoBERTa model for masked language modeling.\n",
    "base_model_name = \"roberta-base\"\n",
    "tokenizer_bm = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    output_hidden_states=True  # Enables output of hidden states for additional tasks.\n",
    ")\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "# Arguments:\n",
    "# - `adapter_path` (str): Path to the fine-tuned LoRA adapter model.\n",
    "# Returns:\n",
    "# - `tokenizer_ftm`: Tokenizer corresponding to the fine-tuned model.\n",
    "# - `fine_tuned_model`: The base model combined with the fine-tuned LoRA adapter.\n",
    "adapter_path = f\"./models/model_{MODEL_VERSION}/fine_tuned_lora_mlm\"\n",
    "tokenizer_ftm = AutoTokenizer.from_pretrained(adapter_path)\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2. Defining models encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders for Embedding Extraction\n",
    "In this section, we implement two functions for extracting text embeddings:\n",
    "1. **`encoder_fine_tuned`**: Uses the fine-tuned model (with LoRA) to generate embeddings. Inputs are processed in batches, which improves efficiency for large datasets.\n",
    "2. **`encoder_base_model`**: Uses the base model (without fine-tuning) to generate embeddings. This method processes inputs individually.\n",
    "\n",
    "#### Notes\n",
    "- **Hidden States**: Both functions extract the model's final hidden states and compute the mean of all tokens to obtain a representation vector for each input.\n",
    "- **Use of `torch.no_grad`**: Ensures that operations are performed without storing gradients, saving memory.\n",
    "- **Input Size**: The maximum size for each input is limited to 128 tokens, truncating as necessary.\n",
    "\n",
    "These functions can be used to compare representations generated by fine-tuned models and base models, providing insights into the improvements brought by fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Function to encode text inputs using the fine-tuned model\n",
    "# Arguments:\n",
    "# - `inputs` (list of str): List of text inputs to encode.\n",
    "# - `tokenizer` (Tokenizer): Tokenizer associated with the fine-tuned model.\n",
    "# - `model` (Model): The fine-tuned model to use for encoding.\n",
    "# - `batch_size` (int): Number of inputs to process in each batch.\n",
    "# Returns:\n",
    "# - `embeddings` (list of numpy arrays): List of embeddings for the input text.\n",
    "def encoder_fine_tuned(inputs, tokenizer, model, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        # Process inputs in batches\n",
    "        batch_inputs = inputs[i:i + batch_size]\n",
    "        tokens = tokenizer(\n",
    "            batch_inputs, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=128\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            hidden_states = outputs.hidden_states  # Extract hidden states from the model\n",
    "            batch_embeddings = hidden_states[-1].mean(dim=1).cpu().numpy()  # Average embeddings over sequence length\n",
    "            embeddings.extend(batch_embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.eval()\n",
    "\n",
    "# Base model encoder\n",
    "# Arguments:\n",
    "# - `inputs` (list[str]): List of text inputs to encode.\n",
    "# - `tokenizer` (AutoTokenizer): Tokenizer corresponding to the base model.\n",
    "# - `model` (AutoModelForMaskedLM): Base model to generate embeddings.\n",
    "# Returns:\n",
    "# - `embeddings` (list[np.array]): List of embeddings generated by the model.\n",
    "def encoder_base_model(inputs, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for input in inputs:\n",
    "        tokens = tokenizer(input, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=tokens['input_ids'])\n",
    "            hidden_states = outputs.hidden_states  # Extract hidden states\n",
    "            embedding = hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()  # Mean of last hidden state\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3. Generate t-SNE graphics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download CWE-to-CVE Mapping Dataset\n",
    "This section downloads a dataset that maps CWE (Common Weakness Enumeration) codes to CVE (Common Vulnerabilities and Exposures) entries.\n",
    "\n",
    "#### Dataset Details\n",
    "- **Source URL**: [Google Drive Link](https://drive.google.com/file/d/1FLnrXFRYinbhttmcZFnqq15t7DAykfJD)\n",
    "- **Local Path**: `./app/data/cwe2cve_dataset.csv`\n",
    "\n",
    "#### Steps\n",
    "1. Check if the directory exists; if not, create it.\n",
    "2. Download the file using the `gdown` library.\n",
    "3. Save the dataset locally for further processing.\n",
    "\n",
    "This dataset is essential for linking vulnerabilities (CVE) to their corresponding weaknesses (CWE), facilitating downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and local file path for the dataset\n",
    "# Arguments:\n",
    "# - `cwe2cve_dataset_url_download` (str): URL to download the CWE-to-CVE mapping dataset.\n",
    "# - `cwe2cve_dataset_path` (str): Local path to save the downloaded dataset.\n",
    "cwe2cve_dataset_url_download = \"https://drive.google.com/uc?id=1FLnrXFRYinbhttmcZFnqq15t7DAykfJD\"\n",
    "cwe2cve_dataset_path = \"./app/data/cwe2cve_dataset.csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(cwe2cve_dataset_path), exist_ok=True)\n",
    "\n",
    "# Download the dataset from Google Drive using gdown\n",
    "gdown.download(cwe2cve_dataset_url_download, cwe2cve_dataset_path, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Embedding Extraction\n",
    "This section processes the CWE-to-CVE dataset to extract embeddings for both base and fine-tuned models.\n",
    "\n",
    "#### Steps\n",
    "1. **Load the dataset**:\n",
    "   - The CWE-to-CVE mapping dataset is loaded from the previously downloaded CSV file.\n",
    "2. **Filter the dataset**:\n",
    "   - Only entries with CWE IDs present in the predefined `cwe_list` are retained.\n",
    "3. **Extract embeddings**:\n",
    "   - **Base Model**: Uses the base model to generate embeddings for the CVE descriptions.\n",
    "   - **Fine-Tuned Model**: Uses the fine-tuned model (with LoRA) to generate embeddings for the same descriptions.\n",
    "\n",
    "#### Outputs\n",
    "- `embedding_base_model`: List of embeddings generated by the base model.\n",
    "- `embedding_fine_tuned_model`: List of embeddings generated by the fine-tuned model.\n",
    "\n",
    "These embeddings can be used for downstream tasks, such as clustering, classification, or comparison of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CWE-to-CVE dataset\n",
    "# Arguments:\n",
    "# - `cwe2cve_dataset_path` (str): Path to the locally downloaded CWE-to-CVE mapping dataset.\n",
    "# Returns:\n",
    "# - `cwe2cve_dataset` (DataFrame): Loaded dataset for further filtering and processing.\n",
    "cwe2cve_dataset = pd.read_csv(cwe2cve_dataset_path)\n",
    "\n",
    "# Filter the dataset to include only rows with CWE IDs from the predefined list\n",
    "# Arguments:\n",
    "# - `cwe_list` (list): List of CWE IDs to filter the dataset.\n",
    "# Returns:\n",
    "# - Filtered DataFrame containing only relevant rows.\n",
    "cwe2cve_dataset = cwe2cve_dataset[cwe2cve_dataset['cwe_id'].isin(cwe_list)]\n",
    "\n",
    "# Extract embeddings using the base model\n",
    "# Arguments:\n",
    "# - `inputs` (list[str]): List of CVE descriptions from the filtered dataset.\n",
    "# - `tokenizer_bm` (AutoTokenizer): Tokenizer for the base model.\n",
    "# - `base_model` (AutoModelForMaskedLM): Base model instance.\n",
    "# Returns:\n",
    "# - `embedding_base_model` (list[np.array]): List of embeddings generated by the base model.\n",
    "embedding_base_model = encoder_base_model(\n",
    "    inputs=cwe2cve_dataset['cve_description'].to_list(), \n",
    "    tokenizer=tokenizer_bm, \n",
    "    model=base_model\n",
    ")\n",
    "\n",
    "# Extract embeddings using the fine-tuned model\n",
    "# Arguments:\n",
    "# - `inputs` (list[str]): List of CVE descriptions from the filtered dataset.\n",
    "# - `tokenizer_ftm` (AutoTokenizer): Tokenizer for the fine-tuned model.\n",
    "# - `fine_tuned_model` (PeftModel): Fine-tuned model instance.\n",
    "# Returns:\n",
    "# - `embedding_fine_tuned_model` (list[np.array]): List of embeddings generated by the fine-tuned model.\n",
    "embedding_fine_tuned_model = encoder_fine_tuned(\n",
    "    inputs=cwe2cve_dataset['cve_description'].to_list(), \n",
    "    tokenizer=tokenizer_ftm, \n",
    "    model=fine_tuned_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings with t-SNE\n",
    "This section visualizes the embeddings generated by the base and fine-tuned models using t-SNE, a dimensionality reduction technique.\n",
    "\n",
    "#### Steps\n",
    "1. **Label Encoding**:\n",
    "   - CWE IDs are converted to numeric values for visualization purposes using `LabelEncoder`.\n",
    "2. **t-SNE Dimensionality Reduction**:\n",
    "   - The embeddings are reduced to two dimensions for both the base and fine-tuned models.\n",
    "   - Parameters:\n",
    "     - `perplexity=30`: Controls the balance between local and global aspects of the data.\n",
    "     - `n_iter=1000`: Number of optimization iterations.\n",
    "3. **Scatter Plot**:\n",
    "   - Each point represents an embedding projected onto two dimensions.\n",
    "   - Colors correspond to different CWE IDs, aiding in cluster identification.\n",
    "\n",
    "#### Outputs\n",
    "- **Left Plot**: t-SNE visualization for the base model embeddings.\n",
    "- **Right Plot**: t-SNE visualization for the fine-tuned model embeddings.\n",
    "\n",
    "#### Observations\n",
    "- Clusters indicate similarities in the embeddings for CWE IDs.\n",
    "- Improved clustering in the fine-tuned model plot may suggest better representation learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CWE labels from the dataset\n",
    "# Arguments:\n",
    "# - `cwe2cve_dataset` (DataFrame): The filtered CWE-to-CVE dataset.\n",
    "# Returns:\n",
    "# - `labels` (np.array): Array of CWE IDs.\n",
    "labels = np.array(cwe2cve_dataset[\"cwe_id\"].tolist())\n",
    "\n",
    "# Convert CWE labels to numeric form using LabelEncoder\n",
    "# Arguments:\n",
    "# - `labels` (list[str]): List of CWE IDs.\n",
    "# Returns:\n",
    "# - `numeric_labels` (np.array): Numeric representation of CWE IDs.\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the embeddings from the base model\n",
    "# Arguments:\n",
    "# - `embedding_base_model` (list[np.array]): List of embeddings from the base model.\n",
    "# Returns:\n",
    "# - `embeddings_tsne_base` (np.array): 2D t-SNE projections of base model embeddings.\n",
    "tsne_base = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "embeddings_tsne_base = tsne_base.fit_transform(np.array(embedding_base_model))\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the embeddings from the fine-tuned model\n",
    "# Arguments:\n",
    "# - `embedding_fine_tuned_model` (list[np.array]): List of embeddings from the fine-tuned model.\n",
    "# Returns:\n",
    "# - `embeddings_tsne_fine_tuned` (np.array): 2D t-SNE projections of fine-tuned model embeddings.\n",
    "tsne_fine_tuned = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "embeddings_tsne_fine_tuned = tsne_fine_tuned.fit_transform(np.array(embedding_fine_tuned_model))\n",
    "\n",
    "# Plotting the t-SNE visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "# t-SNE visualization for the base model\n",
    "scatter_base = axes[0].scatter(\n",
    "    embeddings_tsne_base[:, 0], embeddings_tsne_base[:, 1],\n",
    "    c=numeric_labels, cmap='viridis', alpha=0.7, s=50\n",
    ")\n",
    "axes[0].set_title('t-SNE - Base Model')\n",
    "axes[0].set_xlabel('1st Dimension')\n",
    "axes[0].set_ylabel('2nd Dimension')\n",
    "cbar_base = fig.colorbar(scatter_base, ax=axes[0])\n",
    "cbar_base.set_label('CWE ID')\n",
    "\n",
    "# t-SNE visualization for the fine-tuned model\n",
    "scatter_fine_tuned = axes[1].scatter(\n",
    "    embeddings_tsne_fine_tuned[:, 0], embeddings_tsne_fine_tuned[:, 1],\n",
    "    c=numeric_labels, cmap='viridis', alpha=0.7, s=50\n",
    ")\n",
    "axes[1].set_title('t-SNE - Fine-Tuned Model')\n",
    "axes[1].set_xlabel('1st Dimension')\n",
    "axes[1].set_ylabel('2nd Dimension')\n",
    "cbar_fine_tuned = fig.colorbar(scatter_fine_tuned, ax=axes[1])\n",
    "cbar_fine_tuned.set_label('CWE ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4. Generate vectorial database using Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone Initialization\n",
    "This section initializes the Pinecone client for interacting with Pinecone's vector database.\n",
    "\n",
    "#### Steps\n",
    "1. **Load Environment Variables**:\n",
    "   - The `load_dotenv` function reads environment variables from a `.env` file.\n",
    "   - Ensure the `.env` file contains a valid `PINECONE_API_KEY`.\n",
    "2. **Retrieve API Key**:\n",
    "   - Use `os.getenv(\"PINECONE_API_KEY\")` to securely fetch the API key.\n",
    "3. **Initialize Pinecone**:\n",
    "   - The `Pinecone` client is created using the retrieved API key for future operations.\n",
    "\n",
    "#### Requirements\n",
    "- A `.env` file with the PINECONE_API_KEY value\n",
    "#### Outputs\n",
    "- `pc`: A Pinecone client instance, ready for use with operations like creating or querying vector indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "# This step reads the environment variables defined in the .env file into the script.\n",
    "# Arguments:\n",
    "# - `.env` file: Should contain `PINECONE_API_KEY` with your Pinecone API key.\n",
    "# Returns:\n",
    "# - None, but sets the environment variables accessible via `os.getenv`.\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Pinecone API key from environment variables\n",
    "# Arguments:\n",
    "# - `PINECONE_API_KEY` (str): API key for authenticating with Pinecone.\n",
    "# Returns:\n",
    "# - `pc_api_key` (str): The retrieved API key.\n",
    "pc_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "# Arguments:\n",
    "# - `api_key` (str): Pinecone API key required for authentication.\n",
    "# Returns:\n",
    "# - `pc` (Pinecone): Pinecone client instance to interact with the Pinecone service.\n",
    "pc = Pinecone(api_key=pc_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embedding for CVE Description\n",
    "This section demonstrates how to generate an embedding for a specific CVE description using the fine-tuned model.\n",
    "\n",
    "#### Steps\n",
    "1. **Define the CVE description**:\n",
    "   - A sample description containing references to vulnerabilities and CWE IDs is provided.\n",
    "2. **Generate the embedding**:\n",
    "   - The `encoder_fine_tuned` function processes the description and returns the corresponding embedding vector.\n",
    "3. **Print the embedding dimensionality**:\n",
    "   - The length of the embedding vector indicates the feature space size of the model.\n",
    "\n",
    "#### Outputs\n",
    "- **Embedding Dimensionality**: Represents the number of features in the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding for a CVE description using the fine-tuned model\n",
    "\n",
    "# Define a sample CVE description\n",
    "# This description contains details about vulnerabilities and CWE references.\n",
    "cve_description = (\n",
    "    \"Large language model (LLM) management tool does not validate the format of a digest value (CWE-1287) \"\n",
    "    \"from a private, untrusted model registry, enabling relative path traversal (CWE-23), a.k.a. Probllama\"\n",
    ")\n",
    "\n",
    "# Generate embedding from the CVE description\n",
    "# Arguments:\n",
    "# - `cve_description` (list[str]): A single-item list containing the CVE description.\n",
    "# - `tokenizer_ftm` (AutoTokenizer): Tokenizer for the fine-tuned model.\n",
    "# - `fine_tuned_model` (PeftModel): Fine-tuned model to generate the embedding.\n",
    "# Returns:\n",
    "# - `embedding` (list[np.array]): List containing the embedding vector for the CVE description.\n",
    "embedding = encoder_fine_tuned([cve_description], tokenizer_ftm, fine_tuned_model)\n",
    "\n",
    "# Print the length of the generated embedding\n",
    "# This indicates the dimensionality of the embedding vector.\n",
    "print(f\"Embedding dimensionality: {len(embedding[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone Index Initialization\n",
    "This section demonstrates how to initialize a vector database index in Pinecone for storing and querying embeddings.\n",
    "\n",
    "#### Steps\n",
    "1. **Check Existing Indexes**:\n",
    "   - Lists all existing indexes to ensure the desired index does not already exist.\n",
    "2. **Define Index Specification**:\n",
    "   - Specifies the cloud provider and region for hosting the index.\n",
    "3. **Determine Embedding Dimensionality**:\n",
    "   - Uses the length of the embedding vector to set the index dimension.\n",
    "4. **Create Index**:\n",
    "   - Creates a new index with the specified name, dimension, and similarity metric if it does not already exist.\n",
    "5. **Wait for Index Readiness**:\n",
    "   - Polls the index status until it is ready for use.\n",
    "6. **Describe Index Statistics**:\n",
    "   - Provides details about the index, such as its size and configuration.\n",
    "\n",
    "#### Outputs\n",
    "- **Index Name**: `fine-tuned-vectorial-database`\n",
    "- **Similarity Metric**: `cosine`\n",
    "- **Cloud**: AWS\n",
    "- **Region**: us-east-1\n",
    "\n",
    "The index is now ready for vector insertion and search operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the Pinecone index\n",
    "# Arguments:\n",
    "# - `index_name` (str): Name of the index to be created or accessed.\n",
    "index_name = \"fine-tuned-vectorial-database\"\n",
    "\n",
    "# List existing indexes on the Pinecone service\n",
    "# Arguments:\n",
    "# - None\n",
    "# Returns:\n",
    "# - `existing_indexes` (list[str]): List of names of existing indexes.\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# Define the Pinecone serverless specification\n",
    "# Arguments:\n",
    "# - `cloud` (str): Cloud provider to host the index (e.g., AWS).\n",
    "# - `region` (str): Region where the index will be hosted.\n",
    "# Returns:\n",
    "# - `spec` (ServerlessSpec): Pinecone serverless specification object.\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", \n",
    "    region=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Determine the dimensionality of the embeddings\n",
    "# Arguments:\n",
    "# - `embedding` (list[np.array]): List containing the embedding vector.\n",
    "# Returns:\n",
    "# - `dimension` (int): The length of the embedding vector.\n",
    "dimension = len(embedding[0])\n",
    "\n",
    "# Create the index if it does not already exist\n",
    "if index_name not in existing_indexes:\n",
    "    # Create a new index\n",
    "    # Arguments:\n",
    "    # - `index_name` (str): Name of the new index.\n",
    "    # - `dimension` (int): Dimensionality of the vectors.\n",
    "    # - `metric` (str): Similarity metric for vector search (e.g., cosine).\n",
    "    # - `spec` (ServerlessSpec): Serverless configuration for the index.\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "    # Wait for the index to become ready\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "# Access the index\n",
    "# Arguments:\n",
    "# - `index_name` (str): Name of the index to access.\n",
    "# Returns:\n",
    "# - `index` (Index): Pinecone index instance.\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Allow some time for initialization\n",
    "time.sleep(1)\n",
    "\n",
    "# Describe the index statistics\n",
    "# Arguments:\n",
    "# - None\n",
    "# Returns:\n",
    "# - Index statistics, including size and configuration.\n",
    "stats = index.describe_index_stats()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWE Data Preparation\n",
    "This section processes a dataset to prepare unique CWE details with metadata for further analysis.\n",
    "\n",
    "#### Steps\n",
    "1. **Load Dataset**:\n",
    "   - Load the CWE-to-CVE dataset into a DataFrame.\n",
    "2. **Drop Duplicates**:\n",
    "   - Retain only unique rows based on `cwe_id` and `cwe_description`.\n",
    "3. **Filter by CWE List**:\n",
    "   - Filter the dataset to include only CWEs in the predefined list (`cwe_list`).\n",
    "4. **Add Unique IDs**:\n",
    "   - Assign a unique ID to each row starting from 1.\n",
    "5. **Normalize Descriptions**:\n",
    "   - Remove accents and special characters from `cwe_description` using the `unidecode` library.\n",
    "6. **Generate Metadata**:\n",
    "   - Create a `metadata` column containing dictionaries with CWE ID and description.\n",
    "\n",
    "#### Outputs\n",
    "- **Filtered and Processed DataFrame**:\n",
    "  - Columns:\n",
    "    - `cwe_id`: CWE ID (e.g., CWE-79).\n",
    "    - `cwe_description`: Description of the CWE without accents or special characters.\n",
    "    - `id`: Unique string ID for each row.\n",
    "    - `metadata`: Dictionary containing `id` and `description`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CWE-to-CVE dataset\n",
    "# Arguments:\n",
    "# - `cwe2cve_dataset_path` (str): Path to the dataset file.\n",
    "# Returns:\n",
    "# - `cwe2cve_dataset` (DataFrame): Loaded dataset.\n",
    "cwe2cve_dataset = pd.read_csv(cwe2cve_dataset_path)\n",
    "\n",
    "# Select unique CWE IDs and descriptions\n",
    "# Arguments:\n",
    "# - `cwe2cve_dataset` (DataFrame): Original dataset.\n",
    "# Returns:\n",
    "# - `data` (DataFrame): Dataset with unique CWE IDs and descriptions.\n",
    "data = cwe2cve_dataset[['cwe_id', 'cwe_description']].drop_duplicates()\n",
    "\n",
    "# Filter the dataset to include only CWEs from the predefined list\n",
    "# Arguments:\n",
    "# - `cwe_list` (list): List of CWE IDs to retain.\n",
    "# Returns:\n",
    "# - Filtered `data` DataFrame.\n",
    "cwe_list = [\"CWE-79\", \"CWE-787\", \"CWE-89\", \"CWE-352\", \"CWE-22\", \"CWE-125\", \"CWE-78\", \"CWE-416\", \"CWE-862\", \"CWE-434\",\n",
    "            \"CWE-94\", \"CWE-20\", \"CWE-77\", \"CWE-287\", \"CWE-269\", \"CWE-502\", \"CWE-200\", \"CWE-863\", \"CWE-918\", \"CWE-119\",\n",
    "            \"CWE-476\", \"CWE-798\", \"CWE-190\", \"CWE-400\", \"CWE-306\"]\n",
    "data = data[data['cwe_id'].isin(cwe_list)].reset_index(drop=True)\n",
    "\n",
    "# Assign unique IDs to each row\n",
    "# Arguments:\n",
    "# - `data` (DataFrame): Filtered dataset.\n",
    "# Returns:\n",
    "# - Updated `data` DataFrame with an 'id' column.\n",
    "data['id'] = range(1, len(data) + 1)\n",
    "data['id'] = data['id'].astype(str)\n",
    "\n",
    "# Remove accents and special characters from descriptions\n",
    "# Arguments:\n",
    "# - `data['cwe_description']` (Series): CWE descriptions.\n",
    "# Returns:\n",
    "# - Updated descriptions without accents or special characters.\n",
    "data['cwe_description'] = data['cwe_description'].apply(unidecode.unidecode)\n",
    "\n",
    "# Create metadata column\n",
    "# Arguments:\n",
    "# - `data` (DataFrame): Dataset with CWE details.\n",
    "# Returns:\n",
    "# - Updated `data` with a 'metadata' column containing dictionaries.\n",
    "data['metadata'] = data.apply(lambda x: {\n",
    "    \"id\": x[\"cwe_id\"],\n",
    "    \"description\": x[\"cwe_description\"]\n",
    "}, axis=1)\n",
    "\n",
    "# Print the final dataset\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing and Pinecone Index Upsertion\n",
    "This section processes data in batches and upserts embeddings into the Pinecone index.\n",
    "\n",
    "#### Steps\n",
    "1. **Preprocess Queries**:\n",
    "   - Cleans the description strings by converting to lowercase and removing non-alphanumeric characters.\n",
    "2. **Batch Embedding Generation**:\n",
    "   - Processes descriptions in batches using the fine-tuned model to generate embeddings.\n",
    "3. **Upsert Vectors**:\n",
    "   - Prepares the batch data, including IDs, embeddings, and metadata, for insertion into the Pinecone index.\n",
    "\n",
    "#### Parameters\n",
    "- **Batch Size**: `1` (number of items processed per batch).\n",
    "- **Index**: Pinecone index initialized earlier.\n",
    "\n",
    "#### Outputs\n",
    "- **Index Updates**:\n",
    "  - Vectors (ID, embedding, metadata) are upserted into the Pinecone index.\n",
    "\n",
    "#### Notes\n",
    "- **Assertions**:\n",
    "  - Ensures the number of embeddings matches the batch size to avoid inconsistencies.\n",
    "- **Scalability**:\n",
    "  - Batch processing ensures efficient handling of large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and data size for processing\n",
    "batch_size = 1\n",
    "data_size = len(data)\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "# Preprocess a query string for vectorization\n",
    "# Arguments:\n",
    "# - `query` (str): The input query string.\n",
    "# Returns:\n",
    "# - `query` (str): Cleaned and preprocessed query string.\n",
    "def preprocess_query(query: str) -> str:\n",
    "    query = query.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n",
    "    query = ''.join(char for char in query if char.isalnum() or char.isspace())  # Keep alphanumeric and spaces\n",
    "    return query\n",
    "\n",
    "# Process data in batches and upsert into Pinecone index\n",
    "# Arguments:\n",
    "# - `data` (list[dict]): List of data entries, where each entry contains an ID and metadata.\n",
    "# - `batch_size` (int): Number of items to process in each batch.\n",
    "# - `data_size` (int): Total size of the data to process.\n",
    "# - `index` (Index): Pinecone index instance for upserting vectors.\n",
    "for i in tqdm(range(0, len(data[:data_size + 1]), batch_size)):\n",
    "    # Define batch range\n",
    "    i_end = min(len(data), i + batch_size)\n",
    "    batch = data[i:i_end]\n",
    "    \n",
    "    # Preprocess descriptions in the batch\n",
    "    # Arguments:\n",
    "    # - `x[\"description\"]`: Text description from the metadata.\n",
    "    # Returns:\n",
    "    # - `chunks` (list[str]): Preprocessed descriptions.\n",
    "    chunks = [preprocess_query(f'{x[\"description\"]}') for x in batch[\"metadata\"]]\n",
    "    print(chunks)  # Optional: Print preprocessed chunks for debugging\n",
    "    \n",
    "    # Generate embeddings for the batch using the fine-tuned model\n",
    "    # Arguments:\n",
    "    # - `chunks` (list[str]): Preprocessed descriptions.\n",
    "    # - `tokenizer_ftm` (AutoTokenizer): Tokenizer for the fine-tuned model.\n",
    "    # - `fine_tuned_model` (PeftModel): Fine-tuned model instance.\n",
    "    # Returns:\n",
    "    # - `embeds` (list[np.array]): List of embeddings for the batch.\n",
    "    embeds = encoder_base_model(chunks, tokenizer_ftm, fine_tuned_model)\n",
    "    \n",
    "    # Ensure the number of embeddings matches the batch size\n",
    "    assert len(embeds) == (i_end - i), \"Mismatch between embeddings and batch size\"\n",
    "    \n",
    "    # Prepare data for upsertion into Pinecone index\n",
    "    # Arguments:\n",
    "    # - `batch[\"id\"]`: List of IDs for the batch.\n",
    "    # - `embeds` (list[np.array]): List of embeddings for the batch.\n",
    "    # - `batch[\"metadata\"]`: List of metadata corresponding to the batch.\n",
    "    # Returns:\n",
    "    # - `to_upsert` (list[tuple]): List of tuples containing ID, embedding, and metadata.\n",
    "    to_upsert = list(zip(batch[\"id\"], embeds, batch[\"metadata\"]))\n",
    "    \n",
    "    # Upsert the batch into the Pinecone index\n",
    "    # Arguments:\n",
    "    # - `vectors` (list[tuple]): List of vectors to upsert into the index.\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5. Example of a retrival from the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Documents from Pinecone Index\n",
    "This function retrieves documents from the Pinecone index that are most similar to a given query.\n",
    "\n",
    "#### Steps\n",
    "1. **Preprocess Query**:\n",
    "   - The input query string is preprocessed (lowercased and cleaned) using the `preprocess_query` function.\n",
    "2. **Generate Query Embedding**:\n",
    "   - The preprocessed query is encoded into a vector using the base model and tokenizer.\n",
    "3. **Query Pinecone Index**:\n",
    "   - The query vector is used to search the Pinecone index for the top-k most similar vectors.\n",
    "4. **Extract Metadata**:\n",
    "   - Metadata from the matching documents is extracted and returned.\n",
    "\n",
    "#### Parameters\n",
    "- **`query`**: The text query for which similar documents are to be retrieved.\n",
    "- **`top_k`**: Number of top results to return.\n",
    "\n",
    "#### Outputs\n",
    "- **`docs`**: A list of metadata corresponding to the top-k matching documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents from Pinecone index based on a query\n",
    "\n",
    "def get_docs(query: str, top_k: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the top-k documents from the Pinecone index that are most similar to the query.\n",
    "\n",
    "    Arguments:\n",
    "    - `query` (str): The input query string to search for.\n",
    "    - `top_k` (int): The number of top documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - `docs` (list[str]): A list of metadata from the top-k matching documents.\n",
    "    \"\"\"\n",
    "    # Preprocess and encode the query into an embedding\n",
    "    xq = encoder_base_model([preprocess_query(query)], tokenizer_bm, fine_tuned_model)\n",
    "    xq = xq[0].tolist()  # Convert the embedding to a list for querying Pinecone\n",
    "    \n",
    "    # Query the Pinecone index using the embedding\n",
    "    # Arguments:\n",
    "    # - `vector` (list[float]): Query vector generated from the input query.\n",
    "    # - `top_k` (int): Number of top matches to retrieve.\n",
    "    # - `include_metadata` (bool): Include metadata in the query results.\n",
    "    # Returns:\n",
    "    # - `res` (dict): Query results containing matches and their metadata.\n",
    "    res = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    "    \n",
    "    # Extract metadata from the matching results\n",
    "    docs = [x[\"metadata\"] for x in res[\"matches\"]]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Pinecone Index and Displaying Results\n",
    "This section demonstrates how to query the Pinecone index for the most similar documents and format the results for display.\n",
    "\n",
    "#### Steps\n",
    "1. **Define the Query**:\n",
    "   - A CVE description is used as the input query.\n",
    "2. **Retrieve Documents**:\n",
    "   - The `get_docs` function retrieves the top-k matching documents based on semantic similarity to the query.\n",
    "3. **Format Results**:\n",
    "   - Each document's ID and description are extracted and formatted for readability.\n",
    "4. **Display Results**:\n",
    "   - The results are joined with a \"---\" separator for clear distinction between documents and printed to the console.\n",
    "\n",
    "#### Outputs\n",
    "- **Formatted Results**:\n",
    "   - Example output format:\n",
    "     ```\n",
    "     Id: CWE-526\n",
    "     Description: A vulnerability in the product allows...\n",
    "     ---\n",
    "     Id: CWE-624\n",
    "     Description: An issue in the authentication module leads...\n",
    "     ```\n",
    "\n",
    "#### Notes\n",
    "- Ensure that the `docs` list contains dictionaries with `id` and `description` keys. \n",
    "- If the metadata structure differs, adjust the key names accordingly in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CVE description to query the Pinecone index\n",
    "cve = \"Product allows attackers to cause a crash via a large number of connections.\"\n",
    "query = cve\n",
    "\n",
    "# Retrieve the top-k matching documents from the index\n",
    "docs = get_docs(query, top_k=5)\n",
    "\n",
    "# Extract titles and descriptions from each document in the retrieved results\n",
    "# Arguments:\n",
    "# - `docs` (list[dict]): List of metadata dictionaries from the matching documents.\n",
    "# Returns:\n",
    "# - `docs_text` (list[str]): List of formatted strings containing ID and description.\n",
    "docs_text = [\n",
    "    f\"Id: {doc['id']}\\nDescription: {doc['description']}\" for doc in docs\n",
    "]\n",
    "\n",
    "# Join the results with \"---\" separator for better readability\n",
    "# Arguments:\n",
    "# - `docs_text` (list[str]): List of formatted document strings.\n",
    "# Returns:\n",
    "# - `top_k_responses` (str): Formatted string with \"---\" separating each document.\n",
    "top_k_responses = \"\\n---\\n\".join(docs_text)\n",
    "\n",
    "# Print the results\n",
    "print(top_k_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Downstream task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1. Generate a pair question/response for incontext learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Dataset Preparation and In-Context Examples\n",
    "This section prepares a supervised dataset for training and generates in-context examples for supervised learning tasks.\n",
    "\n",
    "#### Steps\n",
    "1. **Directory Creation**:\n",
    "   - Creates the directory `./datasets/supervised` to store the supervised dataset.\n",
    "2. **Data Splitting**:\n",
    "   - Splits the dataset into training and testing sets with an 80-20 split if the files do not already exist.\n",
    "   - Saves the splits as `train.csv` and `test.csv`.\n",
    "3. **In-Context Example Creation**:\n",
    "   - Formats each row of the training dataset as a question-answer pair.\n",
    "   - Example:\n",
    "     ```\n",
    "     Question: CVE:Description of a vulnerability?\n",
    "     Answer: CWE-ID:Description of the weakness.\n",
    "     ```\n",
    "4. **Joining Examples**:\n",
    "   - Joins the examples with `---` as a separator for better readability and context during training.\n",
    "\n",
    "#### Outputs\n",
    "- **Training and Testing Splits**:\n",
    "  - `train.csv` and `test.csv` saved in `./datasets/supervised`.\n",
    "- **In-Context Examples**:\n",
    "  - Printed question-answer pairs formatted for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for supervised datasets if it does not exist\n",
    "os.makedirs(\"./datasets/supervised\", exist_ok=True)\n",
    "\n",
    "# Check if the supervised dataset files already exist\n",
    "if not any(os.scandir('./datasets/supervised/')):\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    # Arguments:\n",
    "    # - `cwe2cve_dataset` (DataFrame): Dataset containing CVE and CWE mappings.\n",
    "    # - `test_size` (float): Proportion of data for the test set.\n",
    "    # - `random_state` (int): Seed for reproducibility.\n",
    "    # Returns:\n",
    "    # - `train_data` (DataFrame): Training portion of the dataset.\n",
    "    # - `test_data` (DataFrame): Testing portion of the dataset.\n",
    "    train_data, test_data = train_test_split(\n",
    "        cwe2cve_dataset, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Reset the index of the dataframes\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    # Print the number of samples in each split\n",
    "    print(f\"Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")\n",
    "\n",
    "    # Save the splits to CSV files\n",
    "    train_data.to_csv('./datasets/supervised/train.csv', index=False)\n",
    "    test_data.to_csv('./datasets/supervised/test.csv', index=False)\n",
    "\n",
    "else:\n",
    "    # Load the existing training and testing data from CSV files\n",
    "    train_data = pd.read_csv('./datasets/supervised/train.csv')\n",
    "    test_data = pd.read_csv('./datasets/supervised/test.csv')\n",
    "\n",
    "# Create examples for in-context supervised learning\n",
    "# Arguments:\n",
    "# - `train_data` (DataFrame): Training dataset.\n",
    "# Returns:\n",
    "# - `train_examples` (list[str]): List of formatted question-answer examples.\n",
    "train_examples = []\n",
    "for _, row in train_data.iterrows():\n",
    "    question = f\"CVE:{row['cve_description']}?\"\n",
    "    answer = f\"{row['cwe_id']}:{row['cwe_description']}\"\n",
    "    train_examples.append(f\"{question}\\n{answer}\")\n",
    "\n",
    "# Join the examples with \"---\" as a separator for in-context use\n",
    "# Arguments:\n",
    "# - `train_examples` (list[str]): List of question-answer examples.\n",
    "# Returns:\n",
    "# - `in_context_examples` (str): In-context formatted string of examples.\n",
    "in_context_examples = \"\\n---\\n\".join(train_examples)\n",
    "\n",
    "# Print the in-context examples\n",
    "print(in_context_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2. Instantiates a Llama agent to classify CVEs into CWE classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWE Classification using Chatbot and Context-Driven Approach\n",
    "This function leverages a chatbot model (`llama3`) to classify CVEs into CWE categories using a provided context.\n",
    "\n",
    "#### Steps\n",
    "1. **Define CWE List**:\n",
    "   - Specifies a fixed list of CWE categories for classification.\n",
    "2. **Prepare Context (RAG)**:\n",
    "   - Formats relevant documents into a readable format using \"---\" separators.\n",
    "3. **System Message**:\n",
    "   - Guides the chatbot to follow strict classification rules, including:\n",
    "     - Context-driven analysis.\n",
    "     - Strict output format: `CWE-<ID>: <Description>`.\n",
    "     - Using only CWEs from the predefined list.\n",
    "4. **Chat Interaction**:\n",
    "   - Sends the query and context to the chatbot with a low temperature for deterministic responses.\n",
    "5. **Return Classification**:\n",
    "   - Outputs the most relevant CWE classification for the given query.\n",
    "\n",
    "#### Notes\n",
    "- The system ensures consistency and avoids extraneous explanations or commentary in the output.\n",
    "- Only the specified CWEs are used for classification to maintain focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a CWE classification based on a CVE query and RAG context\n",
    "def generate(query: str, docs: list[dict]):\n",
    "    \"\"\"\n",
    "    Classifies a CVE query to the most relevant CWE category using a context-driven approach.\n",
    "\n",
    "    Arguments:\n",
    "    - `query` (str): CVE description to classify.\n",
    "    - `docs` (list[dict]): List of relevant documents retrieved from the Pinecone index.\n",
    "\n",
    "    Returns:\n",
    "    - `chat_response[\"message\"][\"content\"]` (str): The classified CWE in the strict output format.\n",
    "    \"\"\"\n",
    "\n",
    "    # List of predefined CWE categories for classification\n",
    "    cwe_list = [\n",
    "        \"CWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')\",\n",
    "        \"CWE-787: Out-of-bounds Write\",\n",
    "        \"CWE-89: Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')\",\n",
    "        \"CWE-352: Cross-Site Request Forgery (CSRF)\",\n",
    "        \"CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\",\n",
    "        \"CWE-125: Out-of-bounds Read\",\n",
    "        \"CWE-78: Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')\",\n",
    "        \"CWE-416: Use After Free\",\n",
    "        \"CWE-862: Missing Authorization\",\n",
    "        \"CWE-434: Unrestricted Upload of File with Dangerous Type\",\n",
    "        \"CWE-94: Improper Control of Generation of Code ('Code Injection')\",\n",
    "        \"CWE-20: Improper Input Validation\",\n",
    "        \"CWE-77: Improper Neutralization of Special Elements used in a Command ('Command Injection')\",\n",
    "        \"CWE-287: Improper Authentication\",\n",
    "        \"CWE-269: Improper Privilege Management\",\n",
    "        \"CWE-502: Deserialization of Untrusted Data\",\n",
    "        \"CWE-200: Exposure of Sensitive Information to an Unauthorized Actor\",\n",
    "        \"CWE-863: Incorrect Authorization\",\n",
    "        \"CWE-918: Server-Side Request Forgery (SSRF)\",\n",
    "        \"CWE-119: Improper Restriction of Operations within the Bounds of a Memory Buffer\",\n",
    "        \"CWE-476: NULL Pointer Dereference\",\n",
    "        \"CWE-798: Use of Hard-coded Credentials\",\n",
    "        \"CWE-190: Integer Overflow or Wraparound\",\n",
    "        \"CWE-400: Uncontrolled Resource Consumption\",\n",
    "        \"CWE-306: Missing Authentication for Critical Function\"\n",
    "    ]\n",
    "\n",
    "    # Format relevant documents (RAG context)\n",
    "    docs_text = [f\"Id: {doc['id']}\\nDescription: {doc['description']}\" for doc in docs]\n",
    "    rag = \"\\n---\\n\".join(docs_text)\n",
    "\n",
    "    # System message for the chatbot\n",
    "    system_message = (\n",
    "        \"You are a highly skilled cybersecurity expert specializing in the classification of CVE vulnerabilities. \"\n",
    "        \"Your primary objective is to map each CVE to its most relevant CWE category by analyzing the provided context and examples. \"\n",
    "        \"Follow these guidelines to ensure accurate and consistent classifications:\\n\\n\"\n",
    "        \"1. Context-Driven Analysis: Use the provided RAG context as the primary source of information for understanding and categorizing the vulnerabilities.\\n\"\n",
    "        \"2. Example-Based Learning: Refer to the examples provided to identify patterns and logical reasoning used in prior classifications.\\n\"\n",
    "        \"3. Strict Output Format: Always format your response exactly as follows:\\n\"\n",
    "        \"   CWE-<ID>: <Description>\\n\\n\"\n",
    "        \"4. Focus and Precision: Avoid explanations, justifications, or any additional commentary outside the required format.\\n\\n\"\n",
    "        \"5. You should only respond using CWEs from the following list: \"\n",
    "        + \", \".join(f'\"{cwe}\"' for cwe in cwe_list)\n",
    "        + \"\\n\"\n",
    "        \"### Resources Provided:\\n\"\n",
    "        \"CONTEXT:\\n\"\n",
    "        + rag\n",
    "        + \"\\nEXAMPLES:\\n\"\n",
    "        + \"\\n\".join(in_context_examples[0:100])  # Only a portion of examples for brevity\n",
    "    )\n",
    "\n",
    "    # Construct the message for the chatbot\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    # Call the chatbot with the provided messages and options\n",
    "    chat_response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=messages,\n",
    "        options={\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Return the chatbot's classification response\n",
    "    return chat_response[\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3. Evalueta model in test database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWE Classification Evaluation\n",
    "This section evaluates the performance of the CWE classification system on a test dataset.\n",
    "\n",
    "#### Steps\n",
    "1. **Dataset Preparation**:\n",
    "   - A copy of the test dataset (`results`) is created to store predictions.\n",
    "2. **Define the Evaluation Function**:\n",
    "   - The `evaluate` function:\n",
    "     1. Constructs a query and question based on the CVE details (ID and description).\n",
    "     2. Retrieves relevant documents (`docs`) using the `get_docs` function.\n",
    "     3. Generates a CWE classification (`out`) using the `generate` function.\n",
    "     4. Parses and returns the predicted CWE ID and description.\n",
    "3. **Apply the Function**:\n",
    "   - The `evaluate` function is applied to each row in the test dataset using `apply`, with results stored in new columns (`cwe_id_pred` and `cwe_description_pred`).\n",
    "\n",
    "#### Outputs\n",
    "- **Updated Dataset**:\n",
    "  - The `results` DataFrame includes the following new columns:\n",
    "    - `cwe_id_pred`: Predicted CWE ID.\n",
    "    - `cwe_description_pred`: Predicted CWE description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of CWE classification for each row in the test dataset\n",
    "\n",
    "results = test_data.copy()  # Create a copy of the test data for evaluation\n",
    "\n",
    "def evaluate(row):\n",
    "    \"\"\"\n",
    "    Evaluate the CWE classification for a given row in the dataset.\n",
    "\n",
    "    Arguments:\n",
    "    - `row` (pd.Series): A row from the test dataset containing CVE details.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: Predicted CWE ID and CWE description.\n",
    "    \"\"\"\n",
    "    # Construct the query and question for classification\n",
    "    query = f\"{row['cve_id']}: {row['cve_description']}\"\n",
    "    question = f\"Which CWE category best classifies CVE '{row['cve_id']}': {row['cve_description']}?\"\n",
    "    \n",
    "    # Retrieve relevant documents from the Pinecone index\n",
    "    docs = get_docs(query, top_k=10)\n",
    "    \n",
    "    # Generate CWE classification using the chatbot\n",
    "    out = generate(query=question, docs=docs)\n",
    "    print(out)  # Print the chatbot's output for debugging\n",
    "    \n",
    "    # Parse the predicted CWE ID and description\n",
    "    cwe_id_pred, cwe_description_pred = out.split(':', 1)\n",
    "    return pd.Series({\n",
    "        'cwe_id_pred': cwe_id_pred.strip(),\n",
    "        'cwe_description_pred': cwe_description_pred.strip()\n",
    "    })\n",
    "\n",
    "# Apply the evaluation function to each row of the test dataset\n",
    "# Arguments:\n",
    "# - `results` (DataFrame): Copy of the test dataset.\n",
    "# - `evaluate` (function): Function to perform CWE classification.\n",
    "# Returns:\n",
    "# - Updated DataFrame with predicted CWE ID and description.\n",
    "results[['cwe_id_pred', 'cwe_description_pred']] = results.apply(lambda row: evaluate(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate CWE Classification Performance\n",
    "This section evaluates the performance of CWE classification at the class level and overall.\n",
    "\n",
    "#### Steps\n",
    "1. **Filter Valid Predictions**:\n",
    "   - Rows with valid predicted CWE IDs (`CWE-<ID>`) are retained.\n",
    "2. **Extract Labels**:\n",
    "   - True (`y_true`) and predicted (`y_pred`) CWE IDs are extracted as lists.\n",
    "3. **Calculate Class-Level Metrics**:\n",
    "   - For each unique CWE ID (`Class`):\n",
    "     - **Occurrences**: Number of true labels for the class.\n",
    "     - **Correct Predictions**: Number of correctly predicted labels for the class.\n",
    "     - **Accuracy**: Proportion of correct predictions for the class.\n",
    "4. **Summarize Results**:\n",
    "   - Create a DataFrame summarizing accuracy, occurrences, and correct predictions for each class.\n",
    "5. **Overall Statistics**:\n",
    "   - Compute mean accuracy across all classes, total occurrences, and total correct predictions.\n",
    "\n",
    "#### Outputs\n",
    "- **Class-Level Results (`df_results`)**:\n",
    "  - Columns:\n",
    "    - `Class`: CWE ID.\n",
    "    - `Accuracy`: Accuracy for the class.\n",
    "    - `Occurrences`: Total occurrences in true labels.\n",
    "    - `Correct`: Number of correct predictions.\n",
    "- **Overall Statistics**:\n",
    "  - Mean Accuracy.\n",
    "  - Total Occurrences.\n",
    "  - Total Correct Predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results to include only valid CWE predictions\n",
    "# Arguments:\n",
    "# - `results` (DataFrame): DataFrame containing original and predicted CWE classifications.\n",
    "# Returns:\n",
    "# - `df_cleaned` (DataFrame): DataFrame with valid CWE predictions (e.g., matching \"CWE-<ID>\" format).\n",
    "df_cleaned = results[results['cwe_id_pred'].str.match(r\"CWE-\\d+$\", na=False)]\n",
    "\n",
    "# Extract true and predicted CWE IDs as lists\n",
    "# Arguments:\n",
    "# - `df_cleaned` (DataFrame): DataFrame with valid predictions.\n",
    "# Returns:\n",
    "# - `y_true` (list): List of true CWE IDs.\n",
    "# - `y_pred` (list): List of predicted CWE IDs.\n",
    "y_true = df_cleaned['cwe_id'].to_list()\n",
    "y_pred = df_cleaned['cwe_id_pred'].to_list()\n",
    "\n",
    "# Get the sorted list of all unique classes\n",
    "# Arguments:\n",
    "# - `y_true` (list): List of true CWE IDs.\n",
    "# - `y_pred` (list): List of predicted CWE IDs.\n",
    "# Returns:\n",
    "# - `classes` (list): Sorted list of unique CWE IDs from true and predicted values.\n",
    "classes = sorted(set(y_true + y_pred))\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "valores = []\n",
    "for cls in classes:\n",
    "    # Find indices of true labels matching the current class\n",
    "    true_indices = [i for i, label in enumerate(y_true) if label == cls]\n",
    "    \n",
    "    # Count correct predictions for the current class\n",
    "    correct_predictions = sum(y_pred[i] == cls for i in true_indices)\n",
    "    \n",
    "    # Count total occurrences of the current class in true labels\n",
    "    total_true = len(true_indices)\n",
    "    \n",
    "    # Calculate accuracy for the current class\n",
    "    accuracy = correct_predictions / total_true if total_true > 0 else 0\n",
    "    \n",
    "    # Append results to the list\n",
    "    valores.append((cls, accuracy, total_true, correct_predictions))\n",
    "\n",
    "# Create a DataFrame to summarize results\n",
    "# Arguments:\n",
    "# - `valores` (list[tuple]): List containing class, accuracy, occurrences, and correct predictions.\n",
    "# Returns:\n",
    "# - `df_results` (DataFrame): DataFrame summarizing evaluation metrics per class.\n",
    "df_results = pd.DataFrame(valores, columns=[\"Class\", \"Accuracy\", \"Occurrences\", \"Correct\"])\n",
    "df_results = df_results.sort_values(by=\"Class\")\n",
    "df_results = df_results[df_results['Occurrences'] != 0].reset_index(drop=True)\n",
    "\n",
    "# Calculate overall statistics\n",
    "# Mean accuracy across all classes\n",
    "mean_accuracy = df_results['Accuracy'].mean()\n",
    "\n",
    "# Total occurrences of all classes in true labels\n",
    "total_occurences = df_results['Occurrences'].sum()\n",
    "\n",
    "# Total correct predictions across all classes\n",
    "total_correct_predictions = df_results['Correct'].sum()\n",
    "\n",
    "# Print evaluation results\n",
    "print(df_results)\n",
    "\n",
    "print(\"Occurrences:\", total_occurences)\n",
    "print(\"Correct Predictions:\", total_correct_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for CWE Classification\n",
    "This section visualizes the confusion matrix to evaluate the performance of CWE classification.\n",
    "\n",
    "#### Steps\n",
    "1. **Mapping Classes to Indices**:\n",
    "   - Creates a mapping (`class_to_index`) to convert class labels (CWE IDs) into numeric indices.\n",
    "   - Generates the reverse mapping (`index_to_class`) to convert indices back into labels.\n",
    "2. **Convert Labels to Numeric**:\n",
    "   - Transforms true (`y_true`) and predicted (`y_pred`) labels into numeric format using `class_to_index`.\n",
    "3. **Compute Confusion Matrix**:\n",
    "   - Uses `confusion_matrix` from `sklearn.metrics` to compute the confusion matrix from numeric labels.\n",
    "4. **Plot the Confusion Matrix**:\n",
    "   - Uses `ConfusionMatrixDisplay` to visualize the confusion matrix.\n",
    "   - The matrix displays the number of true and predicted instances for each class.\n",
    "\n",
    "#### Outputs\n",
    "- **Confusion Matrix**:\n",
    "  - Rows: Actual CWE classes.\n",
    "  - Columns: Predicted CWE classes.\n",
    "  - Diagonal values represent correct predictions.\n",
    "  - Off-diagonal values represent misclassifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mappings for unique classes to indices and vice-versa\n",
    "# Arguments:\n",
    "# - `unique_classes` (list): Sorted list of unique CWE IDs.\n",
    "# Returns:\n",
    "# - `class_to_index` (dict): Mapping from class label to numeric index.\n",
    "# - `index_to_class` (dict): Mapping from numeric index to class label.\n",
    "unique_classes = sorted(set(y_true + y_pred))\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "index_to_class = {idx: cls for cls, idx in class_to_index.items()}\n",
    "\n",
    "# Convert true and predicted labels to numeric form\n",
    "# Arguments:\n",
    "# - `y_true` (list): List of true CWE labels.\n",
    "# - `y_pred` (list): List of predicted CWE labels.\n",
    "# Returns:\n",
    "# - `y_true_numeric` (list[int]): Numeric representation of true labels.\n",
    "# - `y_pred_numeric` (list[int]): Numeric representation of predicted labels.\n",
    "y_true_numeric = [class_to_index[label] for label in y_true]\n",
    "y_pred_numeric = [class_to_index[label] for label in y_pred]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "# Arguments:\n",
    "# - `y_true_numeric` (list[int]): Numeric true labels.\n",
    "# - `y_pred_numeric` (list[int]): Numeric predicted labels.\n",
    "# Returns:\n",
    "# - `conf_matrix` (ndarray): Confusion matrix.\n",
    "conf_matrix = confusion_matrix(y_true_numeric, y_pred_numeric)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=unique_classes)\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=90)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for CWE Classification\n",
    "This section computes standard evaluation metrics for CWE classification.\n",
    "\n",
    "#### Metrics\n",
    "1. **Accuracy**:\n",
    "   - Proportion of correctly classified instances among all predictions.\n",
    "   - Formula: `(True Positives + True Negatives) / Total Samples`\n",
    "2. **Precision (Weighted)**:\n",
    "   - Proportion of true positive predictions among all positive predictions, weighted by class size.\n",
    "   - Formula (for each class): `TP / (TP + FP)`\n",
    "3. **Recall (Weighted)**:\n",
    "   - Proportion of true positive predictions among all actual positives, weighted by class size.\n",
    "   - Formula (for each class): `TP / (TP + FN)`\n",
    "4. **F1-Score (Weighted)**:\n",
    "   - Harmonic mean of precision and recall, weighted by class size.\n",
    "   - Formula: `2 * (Precision * Recall) / (Precision + Recall)`\n",
    "\n",
    "#### Steps\n",
    "1. **Compute Metrics**:\n",
    "   - `accuracy_score`: Computes overall accuracy.\n",
    "   - `precision_score`: Computes weighted precision.\n",
    "   - `recall_score`: Computes weighted recall.\n",
    "   - `f1_score`: Computes weighted F1-score.\n",
    "2. **Print Results**:\n",
    "   - Metrics are formatted to four decimal places for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "# Arguments:\n",
    "# - `y_true_numeric` (list[int]): Numeric representation of true labels.\n",
    "# - `y_pred_numeric` (list[int]): Numeric representation of predicted labels.\n",
    "# Returns:\n",
    "# - `accuracy` (float): Overall accuracy of the classification.\n",
    "# - `precision_weighted` (float): Weighted precision score.\n",
    "# - `recall_weighted` (float): Weighted recall score.\n",
    "# - `f1_weighted` (float): Weighted F1 score.\n",
    "accuracy = accuracy_score(y_true_numeric, y_pred_numeric)\n",
    "precision_weighted = precision_score(y_true_numeric, y_pred_numeric, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(y_true_numeric, y_pred_numeric, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(y_true_numeric, y_pred_numeric, average='weighted', zero_division=0)\n",
    "\n",
    "# Print the metrics with four decimal places\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (Weighted): {precision_weighted:.4f}\")\n",
    "print(f\"Recall (Weighted): {recall_weighted:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feedforward multilayer ANN for classification task of CVEs - in their vectorization format - into their corresponding CWE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import data for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cwes = 25\n",
    "\n",
    "### Train data\n",
    "# Step 1: Load the CSV file\n",
    "file_path_train = \"./datasets/supervised/train.csv\"\n",
    "data_train = pd.read_csv(file_path_train)\n",
    "\n",
    "# Step 2: Separate X and Y\n",
    "X_train = [s[:-1] if s.endswith('.') else s for s in data_train.iloc[:, 1].values]  # Keep only the CVE description\n",
    "Y_train = data_train.iloc[:, 2:].values  # Latter two columns for targets\n",
    "\n",
    "# Example: Assume these are the unique CWE IDs in your dataset\n",
    "unique_cwe_ids = sorted(set([int(re.search(r'CWE-(\\d{1,4})', row[0]).group(1)) for row in Y_train]))\n",
    "\n",
    "# Create a mapping from CWE ID to an index (0 to 24)\n",
    "cwe_to_index = {cwe: idx for idx, cwe in enumerate(unique_cwe_ids)}\n",
    "\n",
    "# Map the numeric CWE IDs in Y_train to their indices\n",
    "numeric_cwe_train = np.array([int(re.search(r'CWE-(\\d{1,4})', row[0]).group(1)) for row in Y_train])\n",
    "mapped_indices = np.array([cwe_to_index[cwe] for cwe in numeric_cwe_train])\n",
    "\n",
    "# Create one-hot encoded matrix for the 25 unique CWE IDs\n",
    "one_hot_encoded_fixed_train = np.zeros((len(mapped_indices), 25))  # 25 categories\n",
    "\n",
    "# Set the appropriate positions\n",
    "for i, index in enumerate(mapped_indices):\n",
    "    one_hot_encoded_fixed_train[i, index] = 1\n",
    "\n",
    "### Test data\n",
    "# Load test data\n",
    "file_path_test = \"./datasets/supervised/test.csv\"\n",
    "data_test = pd.read_csv(file_path_test)\n",
    "\n",
    "# Step 2: Separate X and Y\n",
    "X_test = [s[:-1] if s.endswith('.') else s for s in data_test.iloc[:, 1].values]  # CVE descriptions\n",
    "Y_test = data_test.iloc[:, 2:].values  # Target columns\n",
    "\n",
    "# Extract numeric part from 'CWE-' using list comprehension and regex\n",
    "numeric_cwe_test = np.array([int(re.search(r'CWE-(\\d{1,4})', row[0]).group(1)) for row in Y_test])\n",
    "\n",
    "# Map numeric CWE IDs to their indices using the training mapping\n",
    "mapped_indices_test = np.array([\n",
    "    cwe_to_index[cwe] if cwe in cwe_to_index else -1  # Use -1 for unknown CWE IDs\n",
    "    for cwe in numeric_cwe_test\n",
    "])\n",
    "\n",
    "# Filter out rows with unknown CWE IDs (-1), if needed\n",
    "valid_indices = mapped_indices_test != -1\n",
    "mapped_indices_test = mapped_indices_test[valid_indices]\n",
    "X_test = np.array(X_test)[valid_indices]\n",
    "\n",
    "# Create one-hot encoded matrix for the 25 unique CWE IDs\n",
    "one_hot_encoded_fixed_test = np.zeros((len(mapped_indices_test), 25))  # 25 categories\n",
    "\n",
    "# Set the appropriate positions\n",
    "for i, index in enumerate(mapped_indices_test):\n",
    "    one_hot_encoded_fixed_test[i, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transform train and test data into their embedding format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for a list of CVE descriptions\n",
    "embeddings_train = encoder_fine_tuned(X_train)\n",
    "embeddings_test = encoder_fine_tuned(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. ANN Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 768               # Input vector size (V_CVE_size)\n",
    "num_classes = number_of_cwes   # Number of output classes (N_CWE)\n",
    "hidden_sizes = [256, 128, 64]  # Sizes of hidden layers\n",
    "activation_function = nn.ReLU  # Activation function to be used\n",
    "batch_size = 64                # Batch size\n",
    "learning_rate = 1e-3           # Learning rate\n",
    "num_epochs = 10000             # Number of training epochs\n",
    "dropout_prob = 0.5             # Dropout probability for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model dynamically\n",
    "layers = []\n",
    "\n",
    "# Input layer\n",
    "layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "layers.append(activation_function())\n",
    "\n",
    "# Hidden layers\n",
    "for i in range(len(hidden_sizes) - 1):\n",
    "    layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "    layers.append(activation_function())\n",
    "    layers.append(nn.Dropout(dropout_prob))\n",
    "\n",
    "# Output layer\n",
    "layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "# Create the sequential model\n",
    "model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Network Model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function definiton\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimization method\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. ANN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "X_train = torch.tensor(embeddings_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(one_hot_encoded_fixed_train, dtype=torch.float32)  # CrossEntropyLoss expects labels of type Long\n",
    "\n",
    "# Create a TensorDataset from X_train and y_train\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "X_test = torch.tensor(embeddings_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(one_hot_encoded_fixed_test, dtype=torch.float32)  # CrossEntropyLoss expects labels of type Long\n",
    "\n",
    "# Create a TensorDataset from X_train and y_train\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store data for training and the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store loss and accuracy for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the GPU, if avaliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and use the GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move the batch to the GPU\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        # Convert one-hot encoded batch_y to class indices\n",
    "        batch_y_indices = torch.argmax(batch_y, dim=1)  # Convert one-hot to class indices\n",
    "\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        # loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, batch_y_indices)  # Pass indices to loss function\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # _, predicted = torch.max(outputs.data, 1)\n",
    "        # total += batch_y.size(0)\n",
    "        # correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        # Get predicted class indices\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Outputs are of shape [batch_size, num_classes]\n",
    "        \n",
    "        # Calculate correct predictions\n",
    "        total += batch_y_indices.size(0)\n",
    "        correct += (predicted == batch_y_indices).sum().item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(model.state_dict(), \"base_model.pth\")\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. ANN analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Loss and Accuracy\n",
    "\n",
    "# Plot Loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy over epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure as a PNG with high DPI\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.savefig(\"training_metrics.png\", dpi=600, bbox_inches='tight')  # Save as PNG with 600 DPI\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot the train loss on the left y-axis\n",
    "ax1.plot(train_losses, 'b-', label='Train Loss', color='blue')\n",
    "ax1.set_xlabel('Epoch', fontsize=16)  # Increase x-axis label font size\n",
    "ax1.set_ylabel('Loss', color='blue', fontsize=20)  # Increase y-axis label font size\n",
    "ax1.tick_params(axis='both', labelsize=12)  # Increase tick label font size\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a second y-axis to plot train accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(train_accuracies, 'g-', label='Train Accuracy', color='green')\n",
    "ax2.set_ylabel('Accuracy (%)', color='green', fontsize=14)  # Increase y-axis label font size\n",
    "ax2.tick_params(axis='both', labelsize=12)  # Increase tick label font size\n",
    "ax2.tick_params(axis='y', labelcolor='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Data and Compute Performance Metrics\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for test_X, test_y in test_loader:\n",
    "        \n",
    "        # Move the test data to the same device as the model\n",
    "        test_X, test_y = test_X.to(device), test_y.to(device)\n",
    "        \n",
    "        outputs = model(test_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        labels_indices = torch.argmax(test_y, dim=1)  # Class indices for true lab\n",
    "        \n",
    "        all_labels.extend(labels_indices.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "test_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and Display Classification Report\n",
    "num_classes = 25\n",
    "class_names = [str(i) for i in range(num_classes)] \n",
    "report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"base_model.pth\")\n",
    "print(\"Model saved as 'base_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
